{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:53:29.428018Z",
     "start_time": "2025-10-16T09:53:29.420736Z"
    }
   },
   "cell_type": "code",
   "source": "root",
   "id": "f6c878159416eee6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/gregruyoga/gmoneycodes/gmsm/gmsm')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:53:36.971323Z",
     "start_time": "2025-10-16T09:53:36.863708Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSED DATA DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "root = Path().resolve().parent.parent\n",
    "data_path = root / 'gmsm' / 'data' / 'processed' / 'daily_data_2015_2024.csv'\n",
    "print(f\"\\nLoading data from: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total observations: {len(df)}\")\n",
    "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. MISSING VALUES CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "if missing.sum() == 0:\n",
    "    print(\"âœ“ No missing values found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. REALIZED VARIANCE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. REALIZED VARIANCE DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rv = df['realized_variance'].dropna()\n",
    "\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(f\"  Count: {len(rv)}\")\n",
    "print(f\"  Mean: {rv.mean():.4f}\")\n",
    "print(f\"  Median: {rv.median():.4f}\")\n",
    "print(f\"  Std: {rv.std():.4f}\")\n",
    "print(f\"  Min: {rv.min():.4f}\")\n",
    "print(f\"  Max: {rv.max():.4f}\")\n",
    "print(f\"  Range: {rv.max() - rv.min():.4f}\")\n",
    "\n",
    "print(f\"\\nPercentiles:\")\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99, 99.5, 99.9]\n",
    "for p in percentiles:\n",
    "    val = np.percentile(rv, p)\n",
    "    print(f\"  {p:5.1f}%: {val:10.4f}\")\n",
    "\n",
    "print(f\"\\nDistribution shape:\")\n",
    "print(f\"  Skewness: {stats.skew(rv):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(rv):.4f}\")\n",
    "\n",
    "# Check for extreme values\n",
    "print(f\"\\nExtreme values check:\")\n",
    "q99 = np.percentile(rv, 99)\n",
    "extreme_count = (rv > q99 * 3).sum()\n",
    "print(f\"  Values > 3 * 99th percentile: {extreme_count} ({extreme_count/len(rv)*100:.2f}%)\")\n",
    "\n",
    "max_val = rv.max()\n",
    "if max_val > 1000:\n",
    "    print(f\"  âš  WARNING: Maximum RV ({max_val:.2f}) is very large!\")\n",
    "    print(f\"  This may cause numerical overflow in exp() operations\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LOG-SCALE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. LOG-SCALE TRANSFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if log transformation helps\n",
    "log_rv = np.log(rv)\n",
    "print(f\"\\nLog(RV) statistics:\")\n",
    "print(f\"  Mean: {log_rv.mean():.4f}\")\n",
    "print(f\"  Median: {log_rv.median():.4f}\")\n",
    "print(f\"  Std: {log_rv.std():.4f}\")\n",
    "print(f\"  Min: {log_rv.min():.4f}\")\n",
    "print(f\"  Max: {log_rv.max():.4f}\")\n",
    "print(f\"  Range: {log_rv.max() - log_rv.min():.4f}\")\n",
    "\n",
    "print(f\"\\nLog(RV) distribution shape:\")\n",
    "print(f\"  Skewness: {stats.skew(log_rv):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(log_rv):.4f}\")\n",
    "\n",
    "# Check if log-normal distribution is appropriate\n",
    "if log_rv.std() < 10:\n",
    "    print(f\"  âœ“ Log(RV) has reasonable std for log-normal modeling\")\n",
    "else:\n",
    "    print(f\"  âš  Log(RV) std is large, may cause numerical issues\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RETURNS ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. RETURNS DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "returns = df['demeaned_log_return'].dropna()\n",
    "\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(f\"  Count: {len(returns)}\")\n",
    "print(f\"  Mean: {returns.mean():.6f}\")\n",
    "print(f\"  Median: {returns.median():.6f}\")\n",
    "print(f\"  Std: {returns.std():.4f}\")\n",
    "print(f\"  Min: {returns.min():.4f}\")\n",
    "print(f\"  Max: {returns.max():.4f}\")\n",
    "\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [1, 5, 25, 50, 75, 95, 99]:\n",
    "    val = np.percentile(returns, p)\n",
    "    print(f\"  {p:5.1f}%: {val:10.4f}\")\n",
    "\n",
    "print(f\"\\nDistribution shape:\")\n",
    "print(f\"  Skewness: {stats.skew(returns):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(returns):.4f}\")\n",
    "\n",
    "# Check for outliers\n",
    "print(f\"\\nOutlier check (>3 std):\")\n",
    "outliers_pos = (returns > returns.mean() + 3*returns.std()).sum()\n",
    "outliers_neg = (returns < returns.mean() - 3*returns.std()).sum()\n",
    "print(f\"  Positive outliers: {outliers_pos} ({outliers_pos/len(returns)*100:.2f}%)\")\n",
    "print(f\"  Negative outliers: {outliers_neg} ({outliers_neg/len(returns)*100:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SCALING RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. SCALING RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check current scale\n",
    "if rv.mean() > 100:\n",
    "    print(\"\\nâš  ISSUE DETECTED: Realized variance is on a large scale\")\n",
    "    print(f\"   Current mean RV: {rv.mean():.2f}\")\n",
    "    print(f\"   Current max RV: {rv.max():.2f}\")\n",
    "\n",
    "    print(\"\\nðŸ“Š RECOMMENDED FIXES:\")\n",
    "\n",
    "    # Option 1: Divide by 100\n",
    "    scaled_rv_1 = rv / 100\n",
    "    print(f\"\\n1. Divide RV by 100:\")\n",
    "    print(f\"   New mean: {scaled_rv_1.mean():.4f}\")\n",
    "    print(f\"   New max: {scaled_rv_1.max():.4f}\")\n",
    "    print(f\"   New std: {scaled_rv_1.std():.4f}\")\n",
    "\n",
    "    # Option 2: Divide by 10000\n",
    "    scaled_rv_2 = rv / 10000\n",
    "    print(f\"\\n2. Divide RV by 10,000:\")\n",
    "    print(f\"   New mean: {scaled_rv_2.mean():.4f}\")\n",
    "    print(f\"   New max: {scaled_rv_2.max():.4f}\")\n",
    "    print(f\"   New std: {scaled_rv_2.std():.4f}\")\n",
    "\n",
    "    # Option 3: Convert to annualized volatility (sqrt(RV * 252))\n",
    "    ann_vol = np.sqrt(rv * 252)\n",
    "    print(f\"\\n3. Convert to annualized volatility % (sqrt(RV * 252)):\")\n",
    "    print(f\"   Mean: {ann_vol.mean():.4f}%\")\n",
    "    print(f\"   Max: {ann_vol.max():.4f}%\")\n",
    "    print(f\"   Std: {ann_vol.std():.4f}%\")\n",
    "\n",
    "    # Option 4: Use log(RV)\n",
    "    print(f\"\\n4. Use log(RV) instead of RV:\")\n",
    "    print(f\"   Mean: {log_rv.mean():.4f}\")\n",
    "    print(f\"   Max: {log_rv.max():.4f}\")\n",
    "    print(f\"   Std: {log_rv.std():.4f}\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ BEST PRACTICE:\")\n",
    "    print(\"   Option 2 (divide by 10,000) is recommended because:\")\n",
    "    print(\"   - Puts RV in variance units (not percentage^2)\")\n",
    "    print(\"   - Results in values mostly between 0-1\")\n",
    "    print(\"   - Prevents numerical overflow in exp() operations\")\n",
    "    print(\"   - Standard in academic literature\")\n",
    "\n",
    "else:\n",
    "    print(\"âœ“ Realized variance scale appears reasonable\")\n",
    "    print(f\"  Mean RV: {rv.mean():.4f}\")\n",
    "    print(f\"  This should not cause numerical issues\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TIME SERIES CHECKS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. TIME SERIES PROPERTIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for gaps in dates\n",
    "df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "date_diffs = df_sorted['date'].diff()\n",
    "gaps = date_diffs[date_diffs > pd.Timedelta(days=5)]\n",
    "\n",
    "print(f\"\\nTemporal coverage:\")\n",
    "print(f\"  Total days: {len(df_sorted)}\")\n",
    "print(f\"  Expected trading days (~252/year): {len(df_sorted) / ((df_sorted['date'].max() - df_sorted['date'].min()).days / 365.25) :.1f} days/year\")\n",
    "print(f\"  Large gaps (>5 days): {len(gaps)}\")\n",
    "\n",
    "if len(gaps) > 0:\n",
    "    print(f\"\\nLargest gaps:\")\n",
    "    for idx in gaps.nlargest(5).index:\n",
    "        print(f\"  {df_sorted.loc[idx-1, 'date'].date()} -> {df_sorted.loc[idx, 'date'].date()} ({date_diffs[idx].days} days)\")\n",
    "\n",
    "# Autocorrelation\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "print(f\"\\nAutocorrelation (lag 1):\")\n",
    "print(f\"  Returns: {returns.autocorr(lag=1):.4f}\")\n",
    "print(f\"  RV: {rv.autocorr(lag=1):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. YEAR-BY-YEAR COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. YEAR-BY-YEAR STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "yearly_stats = df.groupby('year').agg({\n",
    "    'realized_variance': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'demeaned_log_return': ['mean', 'std', 'min', 'max']\n",
    "})\n",
    "\n",
    "print(\"\\nRealized Variance by year:\")\n",
    "print(yearly_stats['realized_variance'].round(2))\n",
    "\n",
    "print(\"\\nReturns by year:\")\n",
    "print(yearly_stats['demeaned_log_return'].round(4))\n",
    "\n",
    "# ============================================================================\n",
    "# 8. CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corr_data = df[['demeaned_log_return', 'realized_variance']].dropna()\n",
    "correlation = corr_data.corr()\n",
    "\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation)\n",
    "\n",
    "# Leverage effect (negative returns -> higher volatility)\n",
    "neg_returns = df[df['demeaned_log_return'] < 0]\n",
    "pos_returns = df[df['demeaned_log_return'] > 0]\n",
    "\n",
    "print(f\"\\nLeverage effect check:\")\n",
    "print(f\"  Mean RV after negative returns: {neg_returns['forward_realized_variance'].mean():.4f}\")\n",
    "print(f\"  Mean RV after positive returns: {pos_returns['forward_realized_variance'].mean():.4f}\")\n",
    "ratio = neg_returns['forward_realized_variance'].mean() / pos_returns['forward_realized_variance'].mean()\n",
    "print(f\"  Ratio: {ratio:.4f}\")\n",
    "if ratio > 1.1:\n",
    "    print(f\"  âœ“ Leverage effect present (neg returns -> higher vol)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "issues = []\n",
    "recommendations = []\n",
    "\n",
    "if rv.max() > 1000:\n",
    "    issues.append(\"RV values are very large (max > 1000)\")\n",
    "    recommendations.append(\"Rescale RV by dividing by 10,000\")\n",
    "\n",
    "if missing.sum() > 0:\n",
    "    issues.append(f\"{missing.sum()} missing values detected\")\n",
    "    recommendations.append(\"Remove or impute missing values\")\n",
    "\n",
    "if len(gaps) > 10:\n",
    "    issues.append(f\"{len(gaps)} large gaps in dates\")\n",
    "    recommendations.append(\"Check data continuity across year boundaries\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nâš  ISSUES FOUND:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No critical issues detected\")\n",
    "    print(\"  Data appears ready for modeling\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTICS COMPLETE\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSED DATA DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Loading data from: /Users/gregruyoga/gmoneycodes/gmsm/gmsm/data/processed/daily_data_2015_2024.csv\n",
      "\n",
      "Dataset info:\n",
      "  Total observations: 2498\n",
      "  Date range: 2015-01-05 00:00:00 to 2024-12-05 00:00:00\n",
      "  Columns: ['date', 'sum_squared_returns', 'n_obs', 'realized_variance', 'daily_log_return_pct', 'year', 'demeaned_log_return', 'forward_realized_variance']\n",
      "\n",
      "================================================================================\n",
      "1. MISSING VALUES\n",
      "================================================================================\n",
      "Series([], dtype: int64)\n",
      "âœ“ No missing values found\n",
      "\n",
      "================================================================================\n",
      "2. REALIZED VARIANCE DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Basic statistics:\n",
      "  Count: 2498\n",
      "  Mean: 1.1232\n",
      "  Median: 0.4303\n",
      "  Std: 3.6995\n",
      "  Min: 0.0111\n",
      "  Max: 90.9579\n",
      "  Range: 90.9468\n",
      "\n",
      "Percentiles:\n",
      "    1.0%:     0.0321\n",
      "    5.0%:     0.0651\n",
      "   10.0%:     0.0937\n",
      "   25.0%:     0.1885\n",
      "   50.0%:     0.4303\n",
      "   75.0%:     0.9953\n",
      "   90.0%:     2.1644\n",
      "   95.0%:     3.4970\n",
      "   99.0%:    11.1747\n",
      "   99.5%:    17.7492\n",
      "   99.9%:    61.7917\n",
      "\n",
      "Distribution shape:\n",
      "  Skewness: 14.5900\n",
      "  Kurtosis: 271.6202\n",
      "\n",
      "Extreme values check:\n",
      "  Values > 3 * 99th percentile: 7 (0.28%)\n",
      "\n",
      "================================================================================\n",
      "3. LOG-SCALE TRANSFORMATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Log(RV) statistics:\n",
      "  Mean: -0.8065\n",
      "  Median: -0.8433\n",
      "  Std: 1.2394\n",
      "  Min: -4.4997\n",
      "  Max: 4.5104\n",
      "  Range: 9.0101\n",
      "\n",
      "Log(RV) distribution shape:\n",
      "  Skewness: 0.2932\n",
      "  Kurtosis: 0.3837\n",
      "  âœ“ Log(RV) has reasonable std for log-normal modeling\n",
      "\n",
      "================================================================================\n",
      "4. RETURNS DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Basic statistics:\n",
      "  Count: 2498\n",
      "  Mean: -0.000075\n",
      "  Median: 0.017818\n",
      "  Std: 1.1242\n",
      "  Min: -12.8527\n",
      "  Max: 8.9721\n",
      "\n",
      "Percentiles:\n",
      "    1.0%:    -3.3174\n",
      "    5.0%:    -1.7248\n",
      "   25.0%:    -0.4160\n",
      "   50.0%:     0.0178\n",
      "   75.0%:     0.5392\n",
      "   95.0%:     1.5064\n",
      "   99.0%:     2.5165\n",
      "\n",
      "Distribution shape:\n",
      "  Skewness: -0.8176\n",
      "  Kurtosis: 16.0725\n",
      "\n",
      "Outlier check (>3 std):\n",
      "  Positive outliers: 11 (0.44%)\n",
      "  Negative outliers: 24 (0.96%)\n",
      "\n",
      "================================================================================\n",
      "5. SCALING RECOMMENDATIONS\n",
      "================================================================================\n",
      "âœ“ Realized variance scale appears reasonable\n",
      "  Mean RV: 1.1232\n",
      "  This should not cause numerical issues\n",
      "\n",
      "================================================================================\n",
      "6. TIME SERIES PROPERTIES\n",
      "================================================================================\n",
      "\n",
      "Temporal coverage:\n",
      "  Total days: 2498\n",
      "  Expected trading days (~252/year): 251.9 days/year\n",
      "  Large gaps (>5 days): 0\n",
      "\n",
      "Autocorrelation (lag 1):\n",
      "  Returns: -0.1326\n",
      "  RV: 0.6895\n",
      "\n",
      "================================================================================\n",
      "7. YEAR-BY-YEAR STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Realized Variance by year:\n",
      "      count  mean    std   min    max\n",
      "year                                 \n",
      "2015    251  0.72   1.44  0.04  19.73\n",
      "2016    252  0.58   0.83  0.02   7.70\n",
      "2017    251  0.18   0.18  0.01   1.16\n",
      "2018    251  1.04   1.67  0.03  16.97\n",
      "2019    252  0.56   0.63  0.02   3.75\n",
      "2020    253  4.09  10.52  0.06  90.96\n",
      "2021    252  0.66   0.75  0.03   4.44\n",
      "2022    251  2.07   1.96  0.02  14.72\n",
      "2023    250  0.67   0.60  0.03   3.55\n",
      "2024    235  0.64   1.58  0.03  22.98\n",
      "\n",
      "Returns by year:\n",
      "        mean     std      min     max\n",
      "year                                 \n",
      "2015 -0.0462  0.9768  -4.0892  3.7532\n",
      "2016 -0.0071  0.8214  -3.6884  2.3167\n",
      "2017  0.0274  0.4148  -1.8526  1.2971\n",
      "2018 -0.0693  1.0755  -3.9624  4.7880\n",
      "2019  0.0573  0.7850  -3.0849  3.3345\n",
      "2020  0.0162  2.1809 -12.8527  8.9721\n",
      "2021  0.0511  0.8202  -2.6096  2.2944\n",
      "2022 -0.1295  1.5158  -4.4769  5.2853\n",
      "2023  0.0435  0.8212  -2.0672  2.1913\n",
      "2024  0.0594  0.7794  -3.0449  2.4373\n",
      "\n",
      "================================================================================\n",
      "8. CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Correlation matrix:\n",
      "                     demeaned_log_return  realized_variance\n",
      "demeaned_log_return             1.000000          -0.175593\n",
      "realized_variance              -0.175593           1.000000\n",
      "\n",
      "Leverage effect check:\n",
      "  Mean RV after negative returns: 1.3615\n",
      "  Mean RV after positive returns: 0.8933\n",
      "  Ratio: 1.5242\n",
      "  âœ“ Leverage effect present (neg returns -> higher vol)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY AND RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "âœ“ No critical issues detected\n",
      "  Data appears ready for modeling\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTICS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "94285f685270c2f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
