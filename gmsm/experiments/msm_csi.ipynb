{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-05T18:29:59.595519Z",
     "start_time": "2025-06-05T18:29:51.429314Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from gmsm.models import MSM\n",
    "import time\n",
    "\n",
    "root = Path().resolve().parent\n",
    "data_path = root / 'data' / 'csi2007.csv'\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df2007 = df[df['Year'] == 2007].loc[:, ['close']]"
   ],
   "id": "de09351cd54610bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df\n",
   "id": "c5210e7d2f307e7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(df2007)",
   "id": "47b45aba24f21191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.loc[:, 'log_returns'] = np.log(df['close']).diff()",
   "id": "9dba408074b040f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_clean = df.dropna(subset=['log_returns'])\n",
    "returns = df_clean['log_returns'].values"
   ],
   "id": "ebfe9520682416ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kbar = 9\n",
    "annualization_factor = len(df2007)\n",
    "initial_estimation_years = 2\n",
    "evaluation_start_year = 2008"
   ],
   "id": "2d821619cd9b9b5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from tqdm.auto import tqdm # More flexible tqdm import\n",
    "import time # To time the process\n",
    "import sys # Import sys for explicit exit\n",
    "# Import traceback for detailed error info\n",
    "import traceback\n",
    "\n",
    "# --- Assume MSM class is defined above or imported ---\n",
    "# >>>>>>>> PASTE YOUR COMPLETE MSM CLASS DEFINITION HERE <<<<<<<<<<<\n",
    "# class MSM:\n",
    "#    ... (Your full MSM class code) ...\n",
    "# >>>>>>>> OR IMPORT IT: <<<<<<<<<<<\n",
    "# from gmsm.modules.msm import MSM # Assuming your structure\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your 5-minute data CSV\n",
    "# >>>>>>>> IMPORTANT: Update this path <<<<<<<<<<<\n",
    "root = Path().resolve().parent # Assuming script is one level down from project root\n",
    "data_path = root / 'data' / 'csi2007.csv' # Path from user's code\n",
    "data_file = data_path\n",
    "\n",
    "# MSM Model Parameters\n",
    "# --- FIX 1: Reduce KBAR significantly ---\n",
    "KBAR = 3 # Start with 3 components (8 states), try 2 if this still fails\n",
    "# --- FIX 2: Use standard annualization factor ---\n",
    "ANNUALIZATION_FACTOR = 252 # Trading days in a year\n",
    "\n",
    "# Rolling Window Parameters\n",
    "INITIAL_ESTIMATION_YEARS = 3 # Years of data for the first fit\n",
    "EVALUATION_START_YEAR = 2008 # Start forecasting from this year\n",
    "MIN_ROLLING_WINDOW_DAYS = 200 # Minimum number of days required in the rolling window\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def calculate_realized_volatility(df_5min_returns, annualization_factor=252):\n",
    "    \"\"\"Calculates daily realized variance and volatility from 5-min returns.\"\"\"\n",
    "    if not isinstance(df_5min_returns.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Input DataFrame must have a DatetimeIndex.\")\n",
    "    df_copy = df_5min_returns.copy()\n",
    "    # Ensure log returns are numeric and handle potential infinities\n",
    "    df_copy['log_ret_5min'] = pd.to_numeric(df_copy['log_ret_5min'], errors='coerce')\n",
    "    df_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_copy.dropna(subset=['log_ret_5min'], inplace=True) # Drop NaNs again after coercion/inf handling\n",
    "\n",
    "    df_copy['log_ret_5min_sq'] = df_copy['log_ret_5min']**2\n",
    "    # Ensure sum only happens if there are valid squared returns for the day\n",
    "    realized_variance_daily = df_copy['log_ret_5min_sq'].resample('D').sum(min_count=1) # Require at least 1 valid sq return\n",
    "    realized_volatility_daily = np.sqrt(realized_variance_daily * annualization_factor)\n",
    "    realized_volatility_daily.name = 'realized_volatility'\n",
    "    return realized_volatility_daily\n",
    "\n",
    "def calculate_daily_returns(df_5min, price_col='close'):\n",
    "    \"\"\"Calculates daily log returns from 5-min close prices.\"\"\"\n",
    "    if not isinstance(df_5min.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Input DataFrame must have a DatetimeIndex.\")\n",
    "    # Ensure price column is numeric\n",
    "    df_5min[price_col] = pd.to_numeric(df_5min[price_col], errors='coerce')\n",
    "    # Use 'B' frequency to only get business day closes\n",
    "    daily_close = df_5min[price_col].resample('B').last()\n",
    "    # Forward fill missing daily closes (e.g., holidays) before calculating returns\n",
    "    daily_close = daily_close.ffill()\n",
    "    daily_log_returns = np.log(daily_close).diff()\n",
    "    daily_log_returns.name = 'log_ret_daily'\n",
    "    return daily_log_returns\n",
    "\n",
    "def evaluate_forecasts(comparison_df):\n",
    "    \"\"\"Calculates and prints evaluation metrics.\"\"\"\n",
    "    required_cols = ['realized_volatility', 'msm_forecast']\n",
    "    if not all(col in comparison_df.columns for col in required_cols):\n",
    "        print(\"Error: Missing required columns for evaluation.\")\n",
    "        return None\n",
    "    eval_df = comparison_df[required_cols].dropna()\n",
    "    if eval_df.empty or len(eval_df) < 2: # Need at least 2 points for regression\n",
    "        print(\"No overlapping data or insufficient data available for evaluation after dropping NaNs.\")\n",
    "        return None\n",
    "\n",
    "    realized_vol = eval_df['realized_volatility']\n",
    "    forecast_vol = eval_df['msm_forecast']\n",
    "\n",
    "    # Add check for non-finite values before metrics\n",
    "    finite_mask = np.isfinite(realized_vol) & np.isfinite(forecast_vol)\n",
    "    if not finite_mask.all():\n",
    "        print(\"Warning: Non-finite values found in realized or forecast volatility. Filtering before evaluation.\")\n",
    "        realized_vol = realized_vol[finite_mask]\n",
    "        forecast_vol = forecast_vol[finite_mask]\n",
    "        if len(realized_vol) < 2:\n",
    "             print(\"Insufficient finite data points remaining for evaluation.\")\n",
    "             return None\n",
    "\n",
    "\n",
    "    # --- Metrics ---\n",
    "    if len(realized_vol) == 0 or len(forecast_vol) == 0:\n",
    "        print(\"Cannot calculate metrics: No valid data points.\")\n",
    "        return eval_df\n",
    "\n",
    "    try:\n",
    "        mse = mean_squared_error(realized_vol, forecast_vol)\n",
    "        mae = np.mean(np.abs(realized_vol - forecast_vol))\n",
    "        rmse = np.sqrt(mse)\n",
    "        epsilon = 1e-12\n",
    "        realized_var = np.maximum(realized_vol, 0)**2 + epsilon\n",
    "        forecast_var = np.maximum(forecast_vol, 0)**2 + epsilon\n",
    "        forecast_var = np.maximum(forecast_var, epsilon)\n",
    "        valid_qlike_mask = (forecast_var > 0) & (realized_var >= 0)\n",
    "        if np.sum(valid_qlike_mask) > 0:\n",
    "            qlike = np.mean(np.log(forecast_var[valid_qlike_mask]) + realized_var[valid_qlike_mask] / forecast_var[valid_qlike_mask])\n",
    "        else:\n",
    "            qlike = np.nan\n",
    "\n",
    "        print(\"\\n--- Forecast Evaluation Metrics ---\")\n",
    "        print(f\"  RMSE: {rmse:.6f}\")\n",
    "        print(f\"  MAE:  {mae:.6f}\")\n",
    "        print(f\"  MSE:  {mse:.6f}\")\n",
    "        print(f\"  QLIKE:{qlike:.4f} (Lower is better)\")\n",
    "\n",
    "    except Exception as metric_e:\n",
    "        print(f\"Error calculating basic metrics: {metric_e}\")\n",
    "        return eval_df\n",
    "\n",
    "\n",
    "    # --- Mincer-Zarnowitz Regression ---\n",
    "    if np.std(forecast_vol) > 1e-9 and len(forecast_vol) > 1:\n",
    "        X = sm.add_constant(forecast_vol)\n",
    "        y = realized_vol\n",
    "        try:\n",
    "            mz_model = sm.OLS(y, X).fit()\n",
    "            print(\"\\n--- Mincer-Zarnowitz Regression (Realized ~ const + Forecast) ---\")\n",
    "            print(mz_model.summary())\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCould not perform Mincer-Zarnowitz regression: {e}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Mincer-Zarnowitz regression: Insufficient variation in forecasts or too few data points.\")\n",
    "\n",
    "\n",
    "    # --- Plotting ---\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        # Use original eval_df index but filtered data for plotting\n",
    "        plot_index = eval_df.index[finite_mask] if 'finite_mask' in locals() and finite_mask.any() else eval_df.index\n",
    "        plt.plot(plot_index, realized_vol, label='Realized Volatility (Ground Truth)', alpha=0.7, linewidth=1.5, color='black')\n",
    "        plt.plot(plot_index, forecast_vol, label=f'MSM Forecast (k={KBAR})', alpha=0.8, linewidth=1.2, color='red')\n",
    "        plt.title('MSM Daily Volatility Forecast vs. Realized Volatility')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Annualized Volatility')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as plot_e:\n",
    "        print(f\"Error during plotting: {plot_e}\")\n",
    "\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "\n",
    "# 1. Load and Prepare 5-minute Data\n",
    "print(f\"Loading 5-minute data from: {data_file}...\")\n",
    "try:\n",
    "    df_5min = pd.read_csv(\n",
    "        data_file,\n",
    "        index_col='datetime',\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    if df_5min.empty: print(f\"Error: DataFrame is empty after loading data from {data_file}.\"); sys.exit(1)\n",
    "    df_5min.sort_index(inplace=True)\n",
    "    print(f\"Loaded {len(df_5min)} records.\")\n",
    "    print(f\"Data range: {df_5min.index.min()} to {df_5min.index.max()}\")\n",
    "    required_cols_5min = ['close']\n",
    "    if not all(col in df_5min.columns for col in required_cols_5min):\n",
    "         missing_cols = [col for col in required_cols_5min if col not in df_5min.columns]\n",
    "         print(f\"Error: Missing required columns in loaded 5-min data: {missing_cols}\"); sys.exit(1)\n",
    "except FileNotFoundError: print(f\"Error: Data file not found at {data_file}\"); raise\n",
    "except Exception as e: print(f\"Error loading or performing initial checks on data: {e}\"); raise\n",
    "\n",
    "# 2. Calculate 5-min Returns\n",
    "print(\"Calculating 5-minute log returns...\")\n",
    "if 'close' not in df_5min.columns: print(\"Error: 'close' column not found.\"); sys.exit(1)\n",
    "df_5min['close'] = pd.to_numeric(df_5min['close'], errors='coerce')\n",
    "df_5min.dropna(subset=['close'], inplace=True)\n",
    "df_5min['log_ret_5min'] = np.log(df_5min['close']).diff()\n",
    "df_5min = df_5min.dropna(subset=['log_ret_5min'])\n",
    "\n",
    "# 3. Calculate Daily Realized Volatility (Ground Truth)\n",
    "print(\"Calculating daily realized volatility...\")\n",
    "daily_rv = calculate_realized_volatility(df_5min, ANNUALIZATION_FACTOR)\n",
    "\n",
    "# 4. Calculate Daily Returns (for MSM fitting)\n",
    "print(\"Calculating daily log returns...\")\n",
    "daily_returns = calculate_daily_returns(df_5min)\n",
    "\n",
    "# 5. Align Data into a Daily DataFrame\n",
    "print(\"Aligning daily data...\")\n",
    "df_daily = pd.DataFrame(index=daily_returns.index)\n",
    "df_daily = df_daily.join(daily_returns).join(daily_rv)\n",
    "df_daily = df_daily.dropna(subset=['log_ret_daily', 'realized_volatility'])\n",
    "df_daily = df_daily[np.isfinite(df_daily['log_ret_daily'])]\n",
    "print(f\"Created daily DataFrame with {len(df_daily)} trading days.\")\n",
    "print(f\"Daily data range: {df_daily.index.min().date()} to {df_daily.index.max().date()}\")\n",
    "\n",
    "# 6. Rolling Window Forecasting Setup\n",
    "print(\"\\nSetting up rolling window forecasting...\")\n",
    "try:\n",
    "    initial_estimation_start_date = df_daily.index.min()\n",
    "    initial_estimation_end_date_target = initial_estimation_start_date + pd.DateOffset(years=INITIAL_ESTIMATION_YEARS) - pd.Timedelta(days=1)\n",
    "    valid_end_dates = df_daily.index[df_daily.index <= initial_estimation_end_date_target]\n",
    "    if valid_end_dates.empty: raise ValueError(\"Cannot determine initial estimation end date.\")\n",
    "    initial_estimation_end_date = valid_end_dates[-1]\n",
    "\n",
    "    initial_window_data = df_daily.loc[initial_estimation_start_date:initial_estimation_end_date]\n",
    "    initial_window_returns = initial_window_data['log_ret_daily'].dropna().values\n",
    "    initial_window_size_days = len(initial_window_returns) # Use actual returns count\n",
    "    print(f\"Initial estimation window size: {initial_window_size_days} days\")\n",
    "\n",
    "    eval_start_lookup = df_daily.index[df_daily.index >= f\"{EVALUATION_START_YEAR}-01-01\"]\n",
    "    if eval_start_lookup.empty: raise ValueError(f\"No data found starting from {EVALUATION_START_YEAR}\")\n",
    "    evaluation_start_date = eval_start_lookup[0]\n",
    "    evaluation_dates = df_daily.loc[evaluation_start_date:].index\n",
    "    print(f\"Initial estimation period: {initial_estimation_start_date.date()} to {initial_estimation_end_date.date()}\")\n",
    "    print(f\"Evaluation period: {evaluation_start_date.date()} to {df_daily.index.max().date()} ({len(evaluation_dates)} days)\")\n",
    "\n",
    "    if initial_window_size_days < MIN_ROLLING_WINDOW_DAYS:\n",
    "        print(f\"Warning: Initial estimation window ({initial_window_size_days} days) is smaller than MIN_ROLLING_WINDOW_DAYS ({MIN_ROLLING_WINDOW_DAYS}).\")\n",
    "\n",
    "except (IndexError, ValueError) as e: print(f\"Error setting up estimation/evaluation periods: {e}. Check data range and EVALUATION_START_YEAR.\"); raise\n",
    "except Exception as e: print(f\"Unexpected error setting up estimation/evaluation periods: {e}\"); raise\n",
    "\n",
    "# --- DEBUG: Test Fit on Initial Window ---\n",
    "print(\"\\n--- Attempting fit on INITIAL estimation window ---\")\n",
    "if len(initial_window_returns) >= MIN_ROLLING_WINDOW_DAYS and np.all(np.isfinite(initial_window_returns)):\n",
    "    try:\n",
    "        print(f\"Fitting MSM(k={KBAR}) on initial window ({len(initial_window_returns)} days) with verbose=True...\")\n",
    "        initial_model = MSM(ret=initial_window_returns, kbar=KBAR, n_vol=ANNUALIZATION_FACTOR, verbose=True)\n",
    "        if initial_model.parameters is None or not initial_model.results.get('optim_convergence', False):\n",
    "            print(\"--- Initial Fit FAILED or did not converge ---\")\n",
    "            print(f\"   Convergence: {initial_model.results.get('optim_convergence', 'N/A')}\")\n",
    "            print(f\"   Message: {initial_model.results.get('optim_message', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"--- Initial Fit SUCCESSFUL ---\")\n",
    "            print(f\"   LogLik: {initial_model.log_likelihood:.4f}\")\n",
    "            print(f\"   Params (unannualized sigma): {initial_model.parameters}\")\n",
    "            # Optionally try a prediction\n",
    "            # initial_pred = initial_model.predict(h=1)\n",
    "            # print(f\"   Initial 1-step prediction: {initial_pred}\")\n",
    "    except Exception as initial_e:\n",
    "        print(f\"--- ERROR during initial fit attempt: {type(initial_e).__name__} - {initial_e} ---\")\n",
    "        print(traceback.format_exc()) # Print full traceback for the initial fit error\n",
    "else:\n",
    "    print(f\"--- Skipping initial fit test: Window size {len(initial_window_returns)} < {MIN_ROLLING_WINDOW_DAYS} or non-finite data ---\")\n",
    "# --- End DEBUG Block ---\n",
    "\n",
    "\n",
    "print(\"\\nStarting rolling window forecasting loop...\")\n",
    "msm_forecasts = []\n",
    "forecast_dates = []\n",
    "failed_fits = 0 # Counter for failed MSM fits\n",
    "start_time = time.time()\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "for i in tqdm(range(len(evaluation_dates)), desc=\"Rolling Forecast\"):\n",
    "    forecast_for_date = evaluation_dates[i]\n",
    "    forecast_value = np.nan # Default forecast value if anything fails\n",
    "\n",
    "    try:\n",
    "        # --- Window Selection ---\n",
    "        window_end_idx_loc = df_daily.index.get_loc(forecast_for_date) - 1\n",
    "        if window_end_idx_loc < 0: continue\n",
    "\n",
    "        window_start_idx_loc = max(0, window_end_idx_loc - initial_window_size_days + 1)\n",
    "        current_window_data = df_daily.iloc[window_start_idx_loc : window_end_idx_loc + 1]\n",
    "        current_returns = current_window_data['log_ret_daily'].dropna().values\n",
    "\n",
    "        # --- Check Window Size ---\n",
    "        if len(current_returns) < MIN_ROLLING_WINDOW_DAYS:\n",
    "            msm_forecasts.append(np.nan)\n",
    "            forecast_dates.append(forecast_for_date)\n",
    "            continue\n",
    "\n",
    "        # --- Attempt MSM Fit ---\n",
    "        rolling_model = None\n",
    "        if not np.all(np.isfinite(current_returns)):\n",
    "             failed_fits += 1\n",
    "             forecast_value = np.nan\n",
    "        else:\n",
    "            # --- Fit Model (verbose=False in loop) ---\n",
    "            rolling_model = MSM(ret=current_returns, kbar=KBAR, n_vol=ANNUALIZATION_FACTOR, verbose=False)\n",
    "\n",
    "            # --- Check Fit Success ---\n",
    "            if rolling_model.parameters is None or not rolling_model.results.get('optim_convergence', False):\n",
    "                failed_fits += 1\n",
    "                forecast_value = np.nan\n",
    "            else:\n",
    "                # --- Attempt Prediction ---\n",
    "                prediction = rolling_model.predict(h=1)\n",
    "                if prediction and 'vol' in prediction and not np.isnan(prediction['vol'][0, 0]):\n",
    "                    forecast_value = prediction['vol'][0, 0]\n",
    "                else:\n",
    "                    forecast_value = np.nan # Ensure NaN if predict fails\n",
    "\n",
    "    except Exception as e:\n",
    "        # --- Log the specific error for this iteration ---\n",
    "        # Only print first few errors to avoid flooding output\n",
    "        if failed_fits < 10:\n",
    "             print(f\"\\nERROR during loop for {forecast_for_date.date()}: {type(e).__name__} - {e}\")\n",
    "             # Optionally print traceback for first error\n",
    "             # if failed_fits == 0:\n",
    "             #    traceback.print_exc()\n",
    "        elif failed_fits == 10:\n",
    "             print(\"\\n...(suppressing further error messages from loop)...\")\n",
    "\n",
    "        failed_fits += 1\n",
    "        forecast_value = np.nan\n",
    "\n",
    "    # --- Store Results for this date ---\n",
    "    msm_forecasts.append(forecast_value)\n",
    "    forecast_dates.append(forecast_for_date)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nRolling forecast loop finished in {end_time - start_time:.2f} seconds.\")\n",
    "if failed_fits > 0:\n",
    "    print(f\"Warning: The MSM model fit failed, did not converge, or encountered an error for {failed_fits} out of {len(evaluation_dates)} windows.\")\n",
    "\n",
    "# 7. Combine Forecasts and Evaluate\n",
    "print(\"Combining forecasts with daily data...\")\n",
    "if not forecast_dates: print(\"Error: No forecast dates were generated.\"); sys.exit(1)\n",
    "forecast_series = pd.Series(msm_forecasts, index=pd.Index(forecast_dates, name='datetime'), name='msm_forecast')\n",
    "if forecast_series.index.has_duplicates:\n",
    "    print(\"Warning: Duplicate forecast dates found. Keeping last.\"); forecast_series = forecast_series[~forecast_series.index.duplicated(keep='last')]\n",
    "df_daily_eval = pd.merge(df_daily, forecast_series, left_index=True, right_index=True, how='left')\n",
    "df_eval_period = df_daily_eval.loc[evaluation_start_date:].copy()\n",
    "valid_forecast_count = df_eval_period['msm_forecast'].count()\n",
    "print(f\"Generated {valid_forecast_count} valid forecasts out of {len(df_eval_period)} evaluation days.\")\n",
    "\n",
    "if valid_forecast_count > 0:\n",
    "    evaluation_results_df = evaluate_forecasts(df_eval_period)\n",
    "    if evaluation_results_df is not None: print(\"\\n--- Sample of Forecasts vs. Realized Volatility ---\\n\", evaluation_results_df.head())\n",
    "    else: print(\"\\nEvaluation could not be completed.\")\n",
    "else: print(\"\\nNo valid forecasts were generated, skipping evaluation.\")\n",
    "\n",
    "print(\"\\nPipeline finished.\")"
   ],
   "id": "61756ed0313b720b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a5d2aaa84bf9ff85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Necessary Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm # More flexible tqdm import\n",
    "import time # To time the process\n",
    "import sys # Import sys for explicit exit\n",
    "import traceback # Import traceback for detailed error info\n",
    "\n",
    "# =============================================================================\n",
    "# Markov Switching Multifractal Model (MSM) Class Definition\n",
    "# =============================================================================\n",
    "class MSM:\n",
    "    \"\"\"\n",
    "    MSM from the 2004 Calvert & Fisher Paper\n",
    "    -----------\n",
    "    ret : numpy.ndarray\n",
    "        Input time series of returns (T x 1).\n",
    "    kbar : int\n",
    "        Number of frequency components (volatility cascades).\n",
    "    n_vol : int\n",
    "        Number of trading periods in a year (for annualization).\n",
    "    nw_lag : int\n",
    "        Number of lags for Newey-West standard errors (currently affects gradient calc).\n",
    "    parameters : numpy.ndarray\n",
    "        Estimated model parameters [m0, b, gamma_k, sigma_unannualized].\n",
    "    std_errors : numpy.ndarray\n",
    "        Standard errors of the estimated parameters.\n",
    "    log_likelihood : float\n",
    "        The maximized log-likelihood value of the fitted model.\n",
    "    results : dict\n",
    "        - LL: Log-likelihood.\n",
    "        - LLs: Vector of log-likelihood contributions per time step.\n",
    "        - filtered_probabilities: P(state_t | data_{1:t}).\n",
    "        - smoothed_probabilities: P(state_t | data_{1:T}).\n",
    "        - transition_matrix: Estimated state transition matrix A.\n",
    "        - state_vol_multipliers: Estimated state volatility multipliers g_m.\n",
    "        - component_matrix: Matrix mapping states to component values (Mmat).\n",
    "        - optim_message: Message from the optimizer.\n",
    "        - optim_convergence: Boolean indicating optimizer convergence.\n",
    "        - optim_iter: Number of optimizer iterations.\n",
    "        - parameters: Estimated parameters [m0, b, gamma_k, sigma_unannualized].\n",
    "        - std_errors: Standard errors.\n",
    "        - coefficients: Coefficients ready for display (b is NaN if kbar=1, sigma is annualized).\n",
    "    \"\"\"\n",
    "    def __init__(self, ret, kbar=1, n_vol=252, para0=None, nw_lag=0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        ret : array-like\n",
    "            Vector or Series of financial returns.\n",
    "        kbar : int, optional\n",
    "            Number of frequency components (volatility cascades), default is 1.\n",
    "            Determines the number of states (2^kbar).\n",
    "        n_vol : int, optional\n",
    "            Number of trading periods in a year (e.g., 252 for daily data),\n",
    "            used for annualizing sigma. Default is 252.\n",
    "        para0 : list or tuple, optional\n",
    "            Initial parameter values [m0, b, gammak, sigma_annualized] for optimization.\n",
    "            If None, default starting values are used.\n",
    "        nw_lag : int, optional\n",
    "            Number of lags for Newey-West adjustment in standard error calculation.\n",
    "            Default is 0 (no adjustment). Note: Implementation currently uses OPG standard errors.\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.n_vol = n_vol\n",
    "        self.nw_lag = nw_lag\n",
    "\n",
    "        # Validate and prepare inputs\n",
    "        checked_inputs = self._check_and_prepare_inputs(ret, kbar, para0)\n",
    "        self.ret = checked_inputs[\"dat\"]\n",
    "        self.kbar = checked_inputs[\"kbar\"]\n",
    "        self.k_states = 2**self.kbar\n",
    "        self._para0 = checked_inputs[\"start_value\"] # Initial guess (sigma annualized)\n",
    "        self._lb = checked_inputs[\"lb\"] # Lower bounds (sigma unannualized)\n",
    "        self._ub = checked_inputs[\"ub\"] # Upper bounds (sigma unannualized)\n",
    "\n",
    "        # De-annualize initial sigma guess for internal use\n",
    "        self._para0[3] = self._para0[3] / np.sqrt(self.n_vol)\n",
    "\n",
    "        # Initialize results attributes\n",
    "        self.parameters = None\n",
    "        self.std_errors = None\n",
    "        self.log_likelihood = None\n",
    "        self.results = {}\n",
    "\n",
    "        # Fit the model upon initialization\n",
    "        # Use try-except for robustness during initialization\n",
    "        try:\n",
    "            self._fit() # <--- Model fitting happens here\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error during initial _fit call: {e}\", RuntimeWarning)\n",
    "            # Ensure attributes are set even if fit fails\n",
    "            self.parameters = None\n",
    "            self.std_errors = None\n",
    "            self.log_likelihood = -np.inf\n",
    "            self.results = {\"optim_convergence\": False, \"optim_message\": f\"Error in _fit: {e}\"}\n",
    "\n",
    "\n",
    "    def _matrix_power(self, A, power):\n",
    "        \"\"\"\n",
    "        Calculate matrix power using numpy's linalg.matrix_power.\n",
    "        Ensures power is a non-negative integer and A is square.\n",
    "        \"\"\"\n",
    "        if not isinstance(power, int) or power < 0:\n",
    "            raise ValueError(\"power must be a non-negative integer.\")\n",
    "        if A.ndim != 2 or A.shape[0] != A.shape[1]:\n",
    "            raise ValueError(\"A must be a square matrix.\")\n",
    "        # Use numpy's efficient matrix power function\n",
    "        return np.linalg.matrix_power(A, power)\n",
    "\n",
    "    def _check_and_prepare_inputs(self, dat, kbar, x0):\n",
    "        \"\"\"\n",
    "        Validate input data and parameters, return processed inputs.\n",
    "        Handles pandas Series/DataFrame, ensures returns are a column vector,\n",
    "        checks for NaNs, validates kbar, sets parameter bounds, and prepares\n",
    "        initial parameter guesses (para0).\n",
    "        \"\"\"\n",
    "        # Convert pandas input to numpy array\n",
    "        if isinstance(dat, (pd.DataFrame, pd.Series)):\n",
    "            dat = dat.values\n",
    "\n",
    "        # Ensure data is a numpy array\n",
    "        if not isinstance(dat, np.ndarray):\n",
    "            dat = np.array(dat)\n",
    "\n",
    "        # Ensure data is a column vector (T x 1)\n",
    "        if dat.ndim == 1:\n",
    "            dat = dat.reshape(-1, 1)\n",
    "        elif dat.shape[1] > 1:\n",
    "            # Warn if multiple columns, use the first one\n",
    "            warnings.warn(\"Input data has multiple columns. Using the first column for returns.\", UserWarning)\n",
    "            dat = dat[:, 0].reshape(-1, 1)\n",
    "\n",
    "        # Check for NaN values in the final return vector\n",
    "        if np.any(np.isnan(dat)):\n",
    "            raise ValueError(\"Input data contains NaN values. Please remove them before passing to the MSM class.\")\n",
    "\n",
    "        # Validate kbar\n",
    "        if kbar < 1 or not isinstance(kbar, int):\n",
    "            raise ValueError('kbar (number of volatility components) must be a positive integer.')\n",
    "\n",
    "        # Define parameter bounds [m0, b, gammak, sigma_unannualized]\n",
    "        # m0: 1 < m0 < 2\n",
    "        # b: b > 1 (only relevant if kbar > 1)\n",
    "        # gammak: 0 < gammak < 1\n",
    "        # sigma: sigma > 0\n",
    "        lb = [1.0 + 1e-6, 1.0 + 1e-6, 1e-6, 1e-6]\n",
    "        ub = [2.0 - 1e-6, 50.0, 1.0 - 1e-6, 50.0] # Allow large b and sigma\n",
    "\n",
    "        # Process initial parameter guess (para0) if provided\n",
    "        if x0 is not None:\n",
    "            if len(x0) != 4:\n",
    "                raise ValueError('Initial values (para0) must be of length 4: [m0, b, gammak, sigma_annualized]')\n",
    "\n",
    "            m0, b_init, gamma_k_init, sigma_ann_init = x0\n",
    "\n",
    "            # Check and clamp initial values to bounds\n",
    "            if not (lb[0] <= m0 <= ub[0]):\n",
    "                warnings.warn(f\"Initial m0 ({m0}) outside bounds. Clamping.\", UserWarning)\n",
    "                m0 = np.clip(m0, lb[0], ub[0])\n",
    "            # Parameter b is only relevant if kbar > 1\n",
    "            if kbar > 1 and not (lb[1] <= b_init <= ub[1]):\n",
    "                warnings.warn(f\"Initial b ({b_init}) outside bounds. Clamping.\", UserWarning)\n",
    "                b_init = np.clip(b_init, lb[1], ub[1])\n",
    "            elif kbar == 1:\n",
    "                b_init = 1.5 # Assign default if kbar=1 (won't be used)\n",
    "            if not (lb[2] <= gamma_k_init <= ub[2]):\n",
    "                warnings.warn(f\"Initial gammak ({gamma_k_init}) outside bounds. Clamping.\", UserWarning)\n",
    "                gamma_k_init = np.clip(gamma_k_init, lb[2], ub[2])\n",
    "            # Ensure initial sigma is positive\n",
    "            if sigma_ann_init <= 0:\n",
    "                warnings.warn(f\"Initial annualized sigma ({sigma_ann_init}) must be positive. Using data std dev.\", UserWarning)\n",
    "                # Calculate default using data std dev\n",
    "                sigma_ann_def_calc = np.std(dat) * np.sqrt(self.n_vol)\n",
    "                # Use calculated std dev or slightly above lower bound if std dev is too small\n",
    "                sigma_ann_init = max(sigma_ann_def_calc, lb[3] * np.sqrt(self.n_vol) * 1.01)\n",
    "\n",
    "            start_value = [m0, b_init, gamma_k_init, sigma_ann_init] # Sigma is annualized here\n",
    "\n",
    "        else:\n",
    "            # Define default initial values if none provided\n",
    "            m0_def = 1.5\n",
    "            b_def = 2.5 if kbar > 1 else 1.5 # Default b > 1 only if needed\n",
    "            gamma_k_def = 0.9\n",
    "            # Calculate annualized std dev from data as initial sigma guess\n",
    "            sigma_ann_def = np.std(dat) * np.sqrt(self.n_vol)\n",
    "            # Ensure default sigma is positive and respects lower bound\n",
    "            # Compare annualized default to annualized version of lower bound\n",
    "            if sigma_ann_def <= (lb[3] * np.sqrt(self.n_vol)):\n",
    "                 sigma_ann_def = lb[3] * np.sqrt(self.n_vol) * 1.1 # Use lower bound (annualized) if std dev is too small\n",
    "\n",
    "            start_value = [m0_def, b_def, gamma_k_def, sigma_ann_def] # Sigma is annualized here\n",
    "            # print(f\"Using default starting values (annualized sigma): {start_value}\") # Removed verbose flag dependency\n",
    "\n",
    "        # Adjust bounds and start value for 'b' if kbar == 1 (parameter b is irrelevant)\n",
    "        if kbar == 1:\n",
    "            lb[1] = 1.5  # Fix b to an arbitrary valid value within bounds\n",
    "            ub[1] = 1.5\n",
    "            start_value[1] = 1.5  # Fix initial b as well\n",
    "\n",
    "        return {\n",
    "            \"dat\": dat,\n",
    "            \"kbar\": kbar,\n",
    "            \"start_value\": start_value,  # sigma is annualized here\n",
    "            \"lb\": lb,  # sigma bound is unannualized\n",
    "            \"ub\": ub  # sigma bound is unannualized\n",
    "        }\n",
    "\n",
    "    def _calculate_transition_matrix(self, b, gamma_kbar):\n",
    "        \"\"\"\n",
    "        Calculates the state transition matrix A based on parameters b and gamma_kbar.\n",
    "        Handles clipping of parameters to ensure numerical stability.\n",
    "        Uses the Kronecker product to build the full matrix for kbar > 1.\n",
    "        Normalizes rows to sum to 1.\n",
    "        \"\"\"\n",
    "        # Clip parameters within the function to ensure validity for calculations\n",
    "        gamma_kbar = np.clip(gamma_kbar, self._lb[2], self._ub[2])\n",
    "        if self.kbar > 1:\n",
    "             b = np.clip(b, self._lb[1], self._ub[1])\n",
    "        else:\n",
    "             b = 1.5 # Set b explicitly if kbar=1\n",
    "\n",
    "        # Initialize gamma_k vector (transition probabilities for each component)\n",
    "        gamma_k = np.zeros(self.kbar)\n",
    "\n",
    "        # Calculate gamma_1 based on gamma_kbar and b\n",
    "        # Avoid log(<=0) or 0^negative_power issues\n",
    "        power_base = max(1.0 - gamma_kbar, 1e-10)\n",
    "        exponent = 1.0 / (b**(self.kbar - 1)) if self.kbar > 1 else 1.0\n",
    "        try:\n",
    "            # Use np.power for potentially fractional exponents safely\n",
    "            gamma_k[0] = 1.0 - np.power(power_base, exponent)\n",
    "        except ValueError: # Handle potential complex result if base is negative (highly unlikely with clip)\n",
    "            gamma_k[0] = 1e-10 # Assign small positive value\n",
    "            warnings.warn(\"Numerical issue calculating gamma_k[0]. Setting to small value.\", RuntimeWarning)\n",
    "\n",
    "        # Ensure gamma_k[0] is within valid probability range [epsilon, 1-epsilon]\n",
    "        gamma_k[0] = np.clip(gamma_k[0], 1e-10, 1.0 - 1e-10)\n",
    "\n",
    "        # Base 2x2 transition matrix for the first component (k=1)\n",
    "        # A_comp = [[P(0->0), P(0->1)], [P(1->0), P(1->1)]]\n",
    "        # P(0->1) = P(1->0) = 0.5 * gamma_k\n",
    "        # P(0->0) = P(1->1) = 1 - 0.5 * gamma_k\n",
    "        A_comp1 = np.array([\n",
    "            [1.0 - 0.5 * gamma_k[0], 0.5 * gamma_k[0]],\n",
    "            [0.5 * gamma_k[0], 1.0 - 0.5 * gamma_k[0]]\n",
    "        ])\n",
    "        A = A_comp1 # Start with the first component's matrix\n",
    "\n",
    "        # Build the full transition matrix using Kronecker product for kbar > 1\n",
    "        if self.kbar > 1:\n",
    "            # Calculate remaining gamma_k values\n",
    "            base_gamma0 = max(1.0 - gamma_k[0], 1e-10) # Use gamma_1 (gamma_k[0])\n",
    "            for i in range(1, self.kbar):\n",
    "                try:\n",
    "                    # gamma_k[i] = 1 - (1 - gamma_k[0])**(b**i)\n",
    "                    gamma_k[i] = 1.0 - np.power(base_gamma0, (b ** i))\n",
    "                except ValueError:\n",
    "                    gamma_k[i] = 1e-10\n",
    "                    warnings.warn(f\"Numerical issue calculating gamma_k[{i}]. Setting to small value.\", RuntimeWarning)\n",
    "\n",
    "                # Clip subsequent gamma_k values\n",
    "                gamma_k[i] = np.clip(gamma_k[i], 1e-10, 1.0 - 1e-10)\n",
    "\n",
    "                # Transition matrix for component i+1\n",
    "                a_i = np.array([\n",
    "                    [1.0 - 0.5 * gamma_k[i], 0.5 * gamma_k[i]],\n",
    "                    [0.5 * gamma_k[i], 1.0 - 0.5 * gamma_k[i]]\n",
    "                ])\n",
    "                # Combine with existing matrix using Kronecker product\n",
    "                A = np.kron(A, a_i)\n",
    "\n",
    "        # Normalize rows to ensure they sum exactly to 1 (handle potential floating point inaccuracies)\n",
    "        row_sums = np.sum(A, axis=1, keepdims=True)\n",
    "        # Avoid division by zero or near-zero\n",
    "        row_sums[row_sums <= 1e-10] = 1.0\n",
    "        A = A / row_sums\n",
    "        # Clip again to handle any minor numerical errors pushing values outside [0, 1]\n",
    "        A = np.clip(A, 0, 1)\n",
    "        # Re-normalize as a final safety check\n",
    "        row_sums = np.sum(A, axis=1, keepdims=True)\n",
    "        row_sums[row_sums <= 1e-10] = 1.0\n",
    "        A = A / row_sums\n",
    "\n",
    "        return A\n",
    "\n",
    "    def _calculate_component_matrix(self, m0):\n",
    "        \"\"\"\n",
    "        Calculates the Mmat matrix (k_states x kbar).\n",
    "        Mmat[i, k] gives the value (m0 or m1=2-m0) of the (k+1)-th\n",
    "        volatility component when the system is in state i.\n",
    "        State i is interpreted as a binary number (b_kbar ... b_1),\n",
    "        where b_k=1 means component k is in the high-volatility state (m1).\n",
    "        \"\"\"\n",
    "        # Clip m0 to ensure m0 and m1 are valid multipliers\n",
    "        m0 = np.clip(m0, self._lb[0], self._ub[0])\n",
    "        m1 = 2.0 - m0\n",
    "        Mmat = np.zeros((self.k_states, self.kbar))\n",
    "\n",
    "        # Iterate through each state (rows)\n",
    "        for i in range(self.k_states):\n",
    "            # Iterate through each component (columns)\n",
    "            for j in range(self.kbar):\n",
    "                # Check the j-th bit of the state index i\n",
    "                # If the j-th bit is 1, component j+1 has value m1\n",
    "                # If the j-th bit is 0, component j+1 has value m0\n",
    "                if (i >> j) & 1:\n",
    "                    Mmat[i, j] = m1\n",
    "                else:\n",
    "                    Mmat[i, j] = m0\n",
    "        return Mmat\n",
    "\n",
    "    def _calculate_state_vol_multipliers(self, m0):\n",
    "        \"\"\"\n",
    "        Calculates the volatility multiplier g_m for each state.\n",
    "        g_m = sqrt(product of component values M_k for that state).\n",
    "        Uses the _calculate_component_matrix method.\n",
    "        Returns a row vector (1 x k_states).\n",
    "        \"\"\"\n",
    "        # Clip m0 first\n",
    "        m0 = np.clip(m0, self._lb[0], self._ub[0])\n",
    "\n",
    "        # Get the matrix of component values for each state\n",
    "        Mmat = self._calculate_component_matrix(m0) # Shape (k_states x kbar)\n",
    "\n",
    "        # Calculate the product of component values along axis 1 (across components) for each state\n",
    "        g_m_sq = np.prod(Mmat, axis=1) # Shape (k_states,)\n",
    "\n",
    "        # Ensure non-negativity before taking the square root (should be guaranteed by m0 bounds)\n",
    "        g_m_sq = np.maximum(g_m_sq, 1e-16) # Use a small floor > 0\n",
    "        g_m = np.sqrt(g_m_sq)\n",
    "\n",
    "        # Return as a row vector (1 x k_states)\n",
    "        return g_m.reshape(1, -1)\n",
    "\n",
    "    def _calculate_conditional_densities(self, params):\n",
    "        \"\"\"\n",
    "        Calculates the conditional probability densities p(ret_t | state_j)\n",
    "        for all time steps t and all states j. Assumes returns are normally\n",
    "        distributed with mean 0 and state-dependent variance.\n",
    "        Uses scipy.stats.norm for calculation.\n",
    "        Returns omega_t matrix (T x k_states).\n",
    "        \"\"\"\n",
    "        # Extract necessary parameters (m0 for multipliers, sigma_unann for scale)\n",
    "        m0, _, _, sigma_unann = params\n",
    "        # Calculate state-dependent volatility multipliers\n",
    "        g_m = self._calculate_state_vol_multipliers(m0)  # Shape (1, k_states)\n",
    "\n",
    "        # Clip sigma_unann to ensure positivity\n",
    "        sigma_unann = np.maximum(sigma_unann, 1e-8)\n",
    "\n",
    "        # Calculate state-dependent standard deviations (unannualized)\n",
    "        state_sigmas = sigma_unann * g_m  # Shape (1, k_states)\n",
    "\n",
    "        # Prepare matrices for vectorized calculation\n",
    "        T = self.ret.shape[0]\n",
    "        # Tile state sigmas T times vertically -> (T, k_states)\n",
    "        sig_mat = np.tile(state_sigmas, (T, 1))\n",
    "        # Tile returns k_states times horizontally -> (T, k_states)\n",
    "        ret_mat = np.tile(self.ret, (1, self.k_states))\n",
    "\n",
    "        # Avoid division by zero or very small sigma in PDF calculation\n",
    "        sig_mat = np.maximum(sig_mat, 1e-16) # Floor sigma values\n",
    "\n",
    "        # Calculate normal PDF using scipy.stats.norm(x, loc=mean, scale=std_dev)\n",
    "        try:\n",
    "            omega_t = stats.norm.pdf(ret_mat, loc=0, scale=sig_mat)\n",
    "        except Exception as e:\n",
    "             # Fallback to manual calculation if stats.norm fails (unlikely)\n",
    "             warnings.warn(f\"Scipy stats.norm.pdf failed: {e}. Using manual calculation.\", RuntimeWarning)\n",
    "             norm_const = 1.0 / (np.sqrt(2 * np.pi) * sig_mat)\n",
    "             exponent = -0.5 * np.square(ret_mat / sig_mat)\n",
    "             # Handle potential overflow in exp\n",
    "             exponent = np.clip(exponent, -700, 700)\n",
    "             omega_t = norm_const * np.exp(exponent)\n",
    "\n",
    "        # Floor the densities to prevent exact zeros (important for log-likelihood)\n",
    "        omega_t = np.maximum(omega_t, 1e-300) # Use a very small floor\n",
    "\n",
    "        return omega_t\n",
    "\n",
    "    def _calculate_likelihood(self, params, return_details=False):\n",
    "        \"\"\"\n",
    "        Calculates the log-likelihood of the model using the Hamilton filter.\n",
    "        Iterates through time, calculating predicted state probabilities,\n",
    "        likelihood of the observation, and updating filtered probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : array-like [m0, b, gammak, sigma_unannualized]\n",
    "        return_details : bool, If True, returns dict with filter details.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float (negative log-likelihood) or dict (if return_details=True).\n",
    "        Handles numerical issues by returning large penalty if errors occur.\n",
    "        \"\"\"\n",
    "        # Clip parameters at the start to ensure they are within bounds\n",
    "        params_clipped = np.clip(params, self._lb, self._ub)\n",
    "        # Special handling for b if kbar=1 (it's fixed during optimization)\n",
    "        if self.kbar == 1:\n",
    "             params_clipped[1] = 1.5\n",
    "\n",
    "        m0, b, gamma_k, sigma_unann = params_clipped\n",
    "        T_obs = self.ret.shape[0]\n",
    "        k_st = self.k_states\n",
    "\n",
    "        # --- Calculate model components (A, g_m, omega_t) ---\n",
    "        try:\n",
    "            A = self._calculate_transition_matrix(b, gamma_k)\n",
    "            g_m = self._calculate_state_vol_multipliers(m0)\n",
    "            omega_t = self._calculate_conditional_densities(params_clipped)\n",
    "        except Exception as e:\n",
    "            # If component calculation fails, return large penalty\n",
    "            warnings.warn(f\"Error during component calculation in likelihood: {e}. Returning large neg LL.\", RuntimeWarning)\n",
    "            fallback_return = {\"pmat\": np.ones((T_obs + 1, k_st)) / k_st, \"LL\": 1e12, \"LLs\": np.full(T_obs, -1e12 / T_obs), \"A\": np.eye(k_st), \"g_m\": np.ones((1, k_st))}\n",
    "            return fallback_return if return_details else 1e12\n",
    "\n",
    "        # --- Hamilton Filter Initialization ---\n",
    "        pmat = np.zeros((T_obs + 1, k_st)) # Filtered probs P(S_t | data_{1:t}), row 0 is initial P(S_0)\n",
    "        LLs = np.zeros(T_obs)              # Log-likelihood contributions log p(y_t | Y_{t-1})\n",
    "\n",
    "        # Initialize P(S_0) with stationary distribution of A if possible, else uniform\n",
    "        try:\n",
    "            eigenvalues, eigenvectors = np.linalg.eig(A.T)\n",
    "            stationary_dist_idx = np.isclose(eigenvalues, 1.0)\n",
    "            if np.sum(stationary_dist_idx) > 0:\n",
    "                # Take the real part of the eigenvector corresponding to eigenvalue 1\n",
    "                stat_dist = eigenvectors[:, stationary_dist_idx].real[:,0]\n",
    "                # Normalize to sum to 1\n",
    "                pmat[0, :] = stat_dist / np.sum(stat_dist)\n",
    "            else: # Fallback if no eigenvalue close to 1 found\n",
    "                 pmat[0, :] = 1.0 / k_st\n",
    "        except np.linalg.LinAlgError: # Fallback for eig decomposition failure\n",
    "            pmat[0, :] = 1.0 / k_st\n",
    "\n",
    "        # Ensure initial distribution is valid (non-negative, sums to 1)\n",
    "        pmat[0, :] = np.maximum(pmat[0, :], 0)\n",
    "        pmat[0, :] = pmat[0, :] / np.sum(pmat[0, :])\n",
    "\n",
    "        # --- Hamilton Filter Loop ---\n",
    "        log_lik_floor = -700 # Approx log(1e-304), floor for log-likelihood contribution\n",
    "\n",
    "        for t in range(T_obs):\n",
    "            # Predict step: P(S_t | data_{1:t-1}) = sum_i P(S_{t-1}=i | data_{1:t-1}) * P(S_t | S_{t-1}=i)\n",
    "            # Ensure pmat[t,:] (P(S_{t-1}|data_{1:t-1})) is valid before matmul\n",
    "            if np.any(np.isnan(pmat[t, :])) or np.sum(pmat[t,:]) <= 1e-10:\n",
    "                 pmat[t, :] = 1.0 / k_st # Reset if invalid state\n",
    "\n",
    "            predicted_prob_st = pmat[t, :] @ A  # Shape (1, k_states)\n",
    "            # Clip predicted probs for numerical stability\n",
    "            predicted_prob_st = np.maximum(predicted_prob_st, 0)\n",
    "            sum_pred = np.sum(predicted_prob_st)\n",
    "            # Normalize predicted probability\n",
    "            if sum_pred > 1e-10:\n",
    "                 predicted_prob_st = predicted_prob_st / sum_pred\n",
    "            else:\n",
    "                 predicted_prob_st = np.ones(k_st) / k_st # Fallback if prediction is zero\n",
    "\n",
    "            # Likelihood of observation y_t: p(y_t | data_{1:t-1}) = sum_j P(S_t=j | data_{1:t-1}) * p(y_t | S_t=j)\n",
    "            # omega_t[t, :] is p(y_t | S_t=j) for j=0..k_states-1\n",
    "            likelihood_yt = np.sum(predicted_prob_st * omega_t[t, :]) # Scalar\n",
    "\n",
    "            # Store log-likelihood contribution, handling potential log(0)\n",
    "            if likelihood_yt <= 1e-300: # Use a very small floor\n",
    "                # warnings.warn(f\"Likelihood near zero at t={t + 1}. Check model/data.\", RuntimeWarning) # Can be noisy\n",
    "                LLs[t] = log_lik_floor\n",
    "                # Reset next state probability to avoid propagation of zeros/NaNs\n",
    "                pmat[t + 1, :] = 1.0 / k_st\n",
    "            else:\n",
    "                LLs[t] = max(np.log(likelihood_yt), log_lik_floor) # Prevent extreme negative LLs\n",
    "\n",
    "                # Update step: P(S_t | data_{1:t}) = P(S_t | data_{1:t-1}) * p(y_t | S_t) / p(y_t | data_{1:t-1})\n",
    "                numerator = predicted_prob_st * omega_t[t, :]  # Element-wise product\n",
    "                pmat[t + 1, :] = numerator / likelihood_yt\n",
    "\n",
    "                # Normalize updated probabilities to ensure they sum to 1\n",
    "                pmat[t + 1, :] = np.maximum(pmat[t + 1, :], 0) # Ensure non-negative\n",
    "                sum_prob = np.sum(pmat[t + 1, :])\n",
    "                if sum_prob > 1e-10:\n",
    "                     pmat[t + 1, :] = pmat[t + 1, :] / sum_prob\n",
    "                else: # If sum is effectively zero (numerical issue), reset to uniform\n",
    "                     pmat[t + 1, :] = 1.0 / k_st\n",
    "\n",
    "        # Calculate total negative log-likelihood\n",
    "        neg_ll = -np.sum(LLs)\n",
    "\n",
    "        # Check for NaN/Inf in final LL (can happen if LLs contains NaNs/Infs)\n",
    "        if not np.isfinite(neg_ll):\n",
    "            warnings.warn(f\"Log-likelihood is not finite ({neg_ll}). Optimization might fail.\", RuntimeWarning)\n",
    "            # Return large penalty, add noise to potentially help optimizer escape\n",
    "            neg_ll = 1e12 + np.random.randn() * 1e6\n",
    "\n",
    "        # Return results\n",
    "        if return_details:\n",
    "            # Ensure pmat is finite before returning\n",
    "            pmat = np.nan_to_num(pmat, nan=1.0/k_st) # Replace NaN with uniform\n",
    "            return {\n",
    "                \"pmat\": pmat,  # Shape (T+1, k_states)\n",
    "                \"LL\": neg_ll,  # Negative Log Likelihood\n",
    "                \"LLs\": LLs,   # Vector of log contributions (T,)\n",
    "                \"A\": A,       # Transition Matrix\n",
    "                \"g_m\": g_m    # State Vol Multipliers\n",
    "            }\n",
    "        else:\n",
    "            # Return only the negative log-likelihood for optimization\n",
    "            return neg_ll\n",
    "\n",
    "    def _log_likelihood_objective(self, params, *args):\n",
    "        \"\"\"\n",
    "        Objective function for the optimizer (scipy.optimize.minimize).\n",
    "        Returns the negative log-likelihood calculated by _calculate_likelihood.\n",
    "        \"\"\"\n",
    "        # Directly call _calculate_likelihood which handles parameter clipping\n",
    "        return self._calculate_likelihood(params, return_details=False)\n",
    "\n",
    "    def _smooth_probabilities(self, A, filtered_pmat):\n",
    "        \"\"\"\n",
    "        Calculates smoothed probabilities P(state_t | data_{1:T}) using Kim's smoother.\n",
    "        Requires the transition matrix (A) and filtered probabilities from Hamilton filter.\n",
    "        Returns smoothed probabilities matrix (T x k_states).\n",
    "        \"\"\"\n",
    "        T_plus_1, k = filtered_pmat.shape  # T_plus_1 = T_obs + 1\n",
    "        T_obs = T_plus_1 - 1\n",
    "\n",
    "        # Check if filtered_pmat contains NaNs - if so, cannot smooth reliably\n",
    "        if np.any(np.isnan(filtered_pmat)):\n",
    "             warnings.warn(\"NaNs detected in filtered probabilities before smoothing. Smoothing may be unreliable. Returning uniform.\", RuntimeWarning)\n",
    "             return np.ones((T_obs, k)) / k\n",
    "\n",
    "        smoothed_p = np.zeros((T_obs, k))  # Smoothed probs for t=1...T_obs\n",
    "\n",
    "        # Smoothed prob at T is just the filtered prob at T\n",
    "        # filtered_pmat[T_obs] contains P(S_T | data_{1:T})\n",
    "        smoothed_p[T_obs - 1, :] = filtered_pmat[T_obs, :]\n",
    "\n",
    "        # Pre-calculate predicted probabilities P(S_{t+1} | data_{1:t}) needed for denominator\n",
    "        predicted_p = np.zeros((T_obs, k))\n",
    "        for t in range(T_obs):\n",
    "            # P(S_{t+1} | data_{1:t}) = sum_i P(S_t=i | data_{1:t}) * P(S_{t+1} | S_t=i)\n",
    "            # filtered_pmat[t+1, :] contains P(S_t | data_{1:t})\n",
    "            pred_step = filtered_pmat[t + 1, :] @ A # Prediction based on *updated* state t prob\n",
    "            pred_step = np.maximum(pred_step, 0)\n",
    "            sum_pred = np.sum(pred_step)\n",
    "            if sum_pred > 1e-10:\n",
    "                 predicted_p[t, :] = pred_step / sum_pred # Store P(S_{t+1} | data_{1:t})\n",
    "            else:\n",
    "                 predicted_p[t, :] = 1.0 / k # Fallback if prediction is zero\n",
    "\n",
    "        # Run smoother backward from T-1 down to 0\n",
    "        # Uses formula: P(S_t|Y_T) = P(S_t|Y_t) .* { A' * [ P(S_{t+1}|Y_T) ./ P(S_{t+1}|Y_t) ] }\n",
    "        # Where .* is element-wise product, ./ is element-wise division\n",
    "        for t in range(T_obs - 2, -1, -1):\n",
    "            # Denominator term: P(S_{t+1} | data_{1:t})\n",
    "            pred_next = predicted_p[t, :]\n",
    "            pred_next = np.maximum(pred_next, 1e-100) # Floor denominator\n",
    "\n",
    "            # Ratio term: P(S_{t+1} | data_{1:T}) / P(S_{t+1} | data_{1:t})\n",
    "            ratio = smoothed_p[t + 1, :] / pred_next # Element-wise division\n",
    "\n",
    "            # Update factor: A' * ratio (matrix multiplication)\n",
    "            update_term = A.T @ ratio # Shape (k,)\n",
    "\n",
    "            # Smoothed probability: P(S_t | data_{1:T})\n",
    "            # filtered_pmat[t+1, :] contains P(S_t | data_{1:t})\n",
    "            smoothed_p[t, :] = filtered_pmat[t + 1, :] * update_term # Element-wise product\n",
    "\n",
    "            # Normalize smoothed probability\n",
    "            smoothed_p[t, :] = np.maximum(smoothed_p[t, :], 0)\n",
    "            sum_smooth = np.sum(smoothed_p[t, :])\n",
    "            if sum_smooth > 1e-10:\n",
    "                smoothed_p[t, :] = smoothed_p[t, :] / sum_smooth\n",
    "            else: # Reset if things go numerically wrong\n",
    "                smoothed_p[t, :] = 1.0 / k\n",
    "\n",
    "        # Final check for NaNs/Infs in smoothed probs\n",
    "        if not np.all(np.isfinite(smoothed_p)):\n",
    "             warnings.warn(\"Non-finite values encountered during smoothing. Resetting to uniform.\", RuntimeWarning)\n",
    "             smoothed_p = np.ones((T_obs, k)) / k\n",
    "\n",
    "        # Return smoothed probabilities for observations t=1...T_obs\n",
    "        return smoothed_p  # Shape (T_obs, k_states)\n",
    "\n",
    "    def _predict_volatility(self, P, A, g_m, sigma_unann, h=None):\n",
    "        \"\"\"\n",
    "        Calculates predicted or fitted conditional volatility/variance.\n",
    "        Uses state probabilities (P), transition matrix (A), multipliers (g_m),\n",
    "        and base sigma. Handles both in-sample fitting (h=None) and\n",
    "        h-step ahead forecasting. Returns annualized values.\n",
    "        \"\"\"\n",
    "        # Check for valid inputs\n",
    "        if P is None or A is None or g_m is None or sigma_unann is None:\n",
    "            warnings.warn(\"Missing inputs for volatility prediction.\", RuntimeWarning)\n",
    "            return {\"vol\": np.nan, \"vol_sq\": np.nan}\n",
    "\n",
    "        # Clip sigma\n",
    "        sigma_unann = np.maximum(sigma_unann, 1e-8)\n",
    "\n",
    "        # --- Forecasting h steps ahead ---\n",
    "        if h is not None:\n",
    "            if not isinstance(h, int) or h < 1:\n",
    "                raise ValueError(\"Forecast horizon h must be a positive integer.\")\n",
    "            # P should be the last filtered probability P(S_T | data_{1:T})\n",
    "            if P.shape[0] != 1:\n",
    "                 raise ValueError(\"For forecasting (h>=1), P must be the last probability vector (1 x k_states).\")\n",
    "\n",
    "            # Calculate P(S_{T+h} | data_{1:T}) = P(S_T | data_{1:T}) @ A^h\n",
    "            try:\n",
    "                A_h = self._matrix_power(A, h)\n",
    "            except ValueError as e: # Handle potential errors in matrix power\n",
    "                 warnings.warn(f\"Error calculating matrix power A^{h}: {e}. Returning NaN forecast.\", RuntimeWarning)\n",
    "                 return {\"vol\": np.array([[np.nan]]), \"vol_sq\": np.array([[np.nan]])}\n",
    "\n",
    "            p_hat = P @ A_h  # Shape (1, k_states)\n",
    "            # Ensure predicted probabilities are valid\n",
    "            p_hat = np.maximum(p_hat, 0)\n",
    "            p_hat_sum = np.sum(p_hat)\n",
    "            if p_hat_sum > 1e-10:\n",
    "                 p_hat = p_hat / p_hat_sum\n",
    "            else:\n",
    "                 p_hat = np.ones_like(p_hat) / p_hat.shape[1] # Uniform if sum is zero\n",
    "\n",
    "\n",
    "        # --- Calculating fitted values (using smoothed or filtered probs) ---\n",
    "        else:\n",
    "            p_hat = P  # Shape (T, k_states)\n",
    "\n",
    "        # Calculate expected squared multiplier E[g_m^2 | Info] = sum_j P(state=j) * g_m(j)^2\n",
    "        g_m_squared = np.square(g_m)  # Shape (1, k_states)\n",
    "        # Matrix multiplication: (T, k) @ (k, 1) -> (T, 1) or (1, k) @ (k, 1) -> (1, 1)\n",
    "        expected_g_m_sq = p_hat @ g_m_squared.T\n",
    "\n",
    "        # Calculate unannualized variance: sigma^2 * E[g_m^2]\n",
    "        vol_sq_unann = (sigma_unann ** 2) * expected_g_m_sq\n",
    "        vol_sq_unann = np.maximum(vol_sq_unann, 1e-16) # Floor variance to prevent negative sqrt\n",
    "\n",
    "        # Annualize variance and calculate volatility\n",
    "        vol_sq_ann = vol_sq_unann * self.n_vol\n",
    "        vol_ann = np.sqrt(vol_sq_ann)\n",
    "\n",
    "        # Ensure results are finite\n",
    "        vol_ann = np.nan_to_num(vol_ann, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
    "        vol_sq_ann = np.nan_to_num(vol_sq_ann, nan=np.nan, posinf=np.nan, neginf=np.nan)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"vol\": vol_ann,     # Annualized Volatility\n",
    "            \"vol_sq\": vol_sq_ann # Annualized Variance\n",
    "        }\n",
    "\n",
    "    def _calculate_gradient(self, params):\n",
    "        \"\"\"\n",
    "        Calculates the gradient (score) of the log-likelihood function\n",
    "        with respect to the parameters [m0, b, gammak, sigma_unann]\n",
    "        using numerical central differences. Required for standard error calculation.\n",
    "        Returns gradient matrix (T x n_params). Handles kbar=1 case for 'b'.\n",
    "        \"\"\"\n",
    "        n_params = len(params)\n",
    "        T = self.ret.shape[0]\n",
    "        grad = np.zeros((T, n_params))\n",
    "\n",
    "        # Step size for numerical differentiation\n",
    "        h = 1e-7 # Slightly larger step size can improve stability\n",
    "        # Calculate absolute step size, respecting bounds (min step h)\n",
    "        h_vec = np.maximum(np.abs(params) * h, h)\n",
    "\n",
    "        for i in range(n_params):\n",
    "            # Skip calculation for 'b' if kbar == 1 (parameter is fixed)\n",
    "            if self.kbar == 1 and i == 1:\n",
    "                grad[:, i] = 0.0\n",
    "                continue\n",
    "\n",
    "            params_fwd = params.copy()\n",
    "            params_bwd = params.copy()\n",
    "\n",
    "            # Apply step\n",
    "            params_fwd[i] += h_vec[i]\n",
    "            params_bwd[i] -= h_vec[i]\n",
    "\n",
    "            # Clip parameters to bounds *after* stepping\n",
    "            params_fwd = np.clip(params_fwd, self._lb, self._ub)\n",
    "            params_bwd = np.clip(params_bwd, self._lb, self._ub)\n",
    "\n",
    "            # Ensure the step actually changed the parameter after clipping\n",
    "            actual_h = params_fwd[i] - params_bwd[i]\n",
    "            if actual_h < 1e-12: # If step is negligible (e.g., param at bound)\n",
    "                 grad[:, i] = 0.0\n",
    "                 continue # Skip gradient calculation for this param\n",
    "\n",
    "            # Calculate LLs for forward and backward steps, handle potential errors\n",
    "            ll_fwd = None\n",
    "            try:\n",
    "                details_fwd = self._calculate_likelihood(params_fwd, return_details=True)\n",
    "                # Check if likelihood calculation failed internally\n",
    "                if np.isfinite(details_fwd[\"LL\"]):\n",
    "                     ll_fwd = details_fwd[\"LLs\"]\n",
    "            except Exception as e:\n",
    "                # warnings.warn(f\"Error calculating forward LL for grad param {i}: {e}. Grad set to 0.\", RuntimeWarning) # Can be noisy\n",
    "                pass # Keep ll_fwd as None\n",
    "\n",
    "            ll_bwd = None\n",
    "            try:\n",
    "                details_bwd = self._calculate_likelihood(params_bwd, return_details=True)\n",
    "                if np.isfinite(details_bwd[\"LL\"]):\n",
    "                     ll_bwd = details_bwd[\"LLs\"]\n",
    "            except Exception as e:\n",
    "                # warnings.warn(f\"Error calculating backward LL for grad param {i}: {e}. Grad set to 0.\", RuntimeWarning)\n",
    "                pass # Keep ll_bwd as None\n",
    "\n",
    "            # If either calculation failed, set gradient to zero for this param\n",
    "            if ll_fwd is None or ll_bwd is None:\n",
    "                grad[:, i] = 0.0\n",
    "                continue\n",
    "\n",
    "            # Ensure LLs vectors are finite for subtraction\n",
    "            ll_fwd = np.nan_to_num(ll_fwd, nan=-700, posinf=0, neginf=-700)\n",
    "            ll_bwd = np.nan_to_num(ll_bwd, nan=-700, posinf=0, neginf=-700)\n",
    "\n",
    "            # Calculate central difference derivative for parameter i\n",
    "            delta_ll = ll_fwd - ll_bwd\n",
    "            grad[:, i] = delta_ll / actual_h\n",
    "\n",
    "        # Check for non-finite values in the final gradient matrix\n",
    "        if not np.all(np.isfinite(grad)):\n",
    "            warnings.warn(\"Non-finite values detected in gradient calculation. Replacing with 0.\", RuntimeWarning)\n",
    "            grad = np.nan_to_num(grad, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        return grad  # Shape (T, n_params)\n",
    "\n",
    "    def _calculate_std_errors(self, params):\n",
    "        \"\"\"\n",
    "        Calculates standard errors for model parameters using the Outer Product\n",
    "        of Gradients (OPG) estimator. SE = sqrt(diag(inv(J))), where J = sum(g_t * g_t').\n",
    "        Handles potential numerical issues during matrix inversion.\n",
    "        Returns standard errors for unannualized parameters (n_params x 1).\n",
    "        \"\"\"\n",
    "        n_params = len(params)\n",
    "        se = np.full(n_params, np.nan) # Initialize with NaNs\n",
    "\n",
    "        try:\n",
    "            # Calculate gradient matrix g (T x n_params)\n",
    "            g = self._calculate_gradient(params)\n",
    "\n",
    "            # Check if gradient calculation resulted in all zeros (indicates problem)\n",
    "            if np.all(np.isclose(g, 0)):\n",
    "                 warnings.warn(\"Gradient matrix is all zeros. Cannot calculate standard errors.\", RuntimeWarning)\n",
    "                 return se.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Outer Product of Gradients (OPG) Information Matrix J\n",
    "            # J = sum_{t=1}^T [ grad_t * grad_t' ] where grad_t is gradient at time t (row vector)\n",
    "            J = g.T @ g  # Shape (n_params x n_params)\n",
    "\n",
    "            # Check if J is singular or ill-conditioned before inverting\n",
    "            # Use condition number or determinant check\n",
    "            cond_num = np.linalg.cond(J)\n",
    "            if cond_num > 1e10: # Check if condition number is very large\n",
    "                 warnings.warn(f\"Information matrix J is ill-conditioned (cond={cond_num:.2e}). Regularizing. SEs may be unreliable.\", RuntimeWarning)\n",
    "                 # Add small value to diagonal for regularization\n",
    "                 cov_matrix = np.linalg.inv(J + np.eye(n_params) * 1e-6)\n",
    "            else:\n",
    "                 cov_matrix = np.linalg.inv(J)\n",
    "\n",
    "            # Extract variances (diagonal elements)\n",
    "            variances = np.diag(cov_matrix)\n",
    "\n",
    "            # Check for negative variances which indicate numerical problems\n",
    "            if np.any(variances < -1e-10): # Allow for tiny negative values due to precision\n",
    "                 warnings.warn(\"Negative variances encountered in SE calculation. Taking absolute value.\", RuntimeWarning)\n",
    "                 variances = np.abs(variances)\n",
    "            # Ensure non-negativity strictly before sqrt\n",
    "            variances = np.maximum(variances, 0)\n",
    "\n",
    "            se = np.sqrt(variances)\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn(\"Could not invert information matrix J (LinAlgError). Standard errors set to NaN.\", RuntimeWarning)\n",
    "            se = np.full(n_params, np.nan)\n",
    "        except Exception as e:\n",
    "             warnings.warn(f\"An unexpected error occurred during standard error calculation: {e}. Standard errors set to NaN.\", RuntimeWarning)\n",
    "             se = np.full(n_params, np.nan)\n",
    "\n",
    "        # Ensure SE for 'b' is NaN if kbar == 1\n",
    "        if self.kbar == 1:\n",
    "            se[1] = np.nan\n",
    "\n",
    "        # Return standard errors for the unannualized parameters\n",
    "        return se.reshape(-1, 1)\n",
    "\n",
    "    def _calculate_marginal_probabilities(self, p, m0, Mmat):\n",
    "        \"\"\"\n",
    "        Calculates marginal probabilities P(M_k=m0 | data) for each component k.\n",
    "        Sums the probabilities of states where component k has value m0.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        p : numpy.ndarray (T x k_states) - Smoothed or filtered state probabilities.\n",
    "        m0 : float - The base multiplier value.\n",
    "        Mmat : numpy.ndarray (k_states x kbar) - Component value matrix.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray (T x kbar) - Marginal probabilities P(M_k=m0 | data).\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if p is None or Mmat is None or not np.all(np.isfinite(p)) or not np.all(np.isfinite(Mmat)):\n",
    "             warnings.warn(\"Invalid inputs for marginal probability calculation.\", RuntimeWarning)\n",
    "             T_fallback = p.shape[0] if p is not None else 1\n",
    "             kbar_fallback = Mmat.shape[1] if Mmat is not None else self.kbar\n",
    "             return np.zeros((T_fallback, kbar_fallback))\n",
    "        if p.shape[1] != self.k_states:\n",
    "            raise ValueError(f\"Prob matrix columns ({p.shape[1]}) != k_states ({self.k_states}).\")\n",
    "        if Mmat.shape[0] != self.k_states or Mmat.shape[1] != self.kbar:\n",
    "            raise ValueError(f\"Mmat shape ({Mmat.shape}) incorrect. Expected ({self.k_states}, {self.kbar}).\")\n",
    "\n",
    "        # Clip m0 just in case\n",
    "        m0 = np.clip(m0, self._lb[0], self._ub[0])\n",
    "\n",
    "        T = p.shape[0]\n",
    "        m_marginals_m0 = np.zeros((T, self.kbar))\n",
    "\n",
    "        # Iterate through each component k (from 0 to kbar-1)\n",
    "        for k in range(self.kbar):\n",
    "            # Find the indices (rows) of states where the k-th component value is m0\n",
    "            # Mmat[:, k] gives the value of component k+1 for all states\n",
    "            states_where_comp_k_is_m0 = np.isclose(Mmat[:, k], m0, atol=1e-5) # Use isclose for float comparison\n",
    "\n",
    "            # Sum the probabilities (p) of these specific states at each time t\n",
    "            # p[:, states_where_comp_k_is_m0] selects columns (states) where M_k is m0\n",
    "            # Summing along axis=1 sums these probabilities for each time t\n",
    "            m_marginals_m0[:, k] = np.sum(p[:, states_where_comp_k_is_m0], axis=1)\n",
    "\n",
    "         # Clip results to be valid probabilities [0, 1]\n",
    "        m_marginals_m0 = np.clip(m_marginals_m0, 0, 1)\n",
    "\n",
    "        return m_marginals_m0 # P(M_k = m0 | data)\n",
    "\n",
    "\n",
    "    def _fit(self):\n",
    "        \"\"\"\n",
    "        Fits the MSM model by maximizing the log-likelihood function using\n",
    "        scipy.optimize.minimize with the L-BFGS-B method (handles bounds).\n",
    "        Stores estimated parameters, standard errors, likelihood, and other\n",
    "        results in the `results` dictionary.\n",
    "        \"\"\"\n",
    "        # Define bounds for the optimizer\n",
    "        bounds = list(zip(self._lb, self._ub))\n",
    "\n",
    "        # Ensure initial parameters are within bounds before starting optimization\n",
    "        para0_clipped = np.clip(self._para0, self._lb, self._ub)\n",
    "        # Ensure b is fixed if kbar=1\n",
    "        if self.kbar == 1:\n",
    "            para0_clipped[1] = 1.5\n",
    "\n",
    "        # Define the objective function (negative log-likelihood)\n",
    "        objective = self._log_likelihood_objective\n",
    "\n",
    "        # print(\"Starting optimization...\") # Removed verbose flag dependency\n",
    "        # Define optimization options\n",
    "        # ftol/gtol control tolerance for termination based on function value/gradient changes\n",
    "        opt_options = {'disp': False, 'maxiter': 500, 'ftol': 1e-9, 'gtol': 1e-5}\n",
    "\n",
    "        opt_result = None # Initialize result\n",
    "        try:\n",
    "            opt_result = minimize(\n",
    "                objective,\n",
    "                para0_clipped, # Use clipped initial guess (unannualized sigma)\n",
    "                args=(),        # No extra args needed for objective\n",
    "                method='L-BFGS-B', # Suitable for bounded problems\n",
    "                bounds=bounds,\n",
    "                options=opt_options\n",
    "            )\n",
    "            # print(f\"Optimization finished: Success={opt_result.success}, Message={opt_result.message}\") # Removed verbose\n",
    "\n",
    "            # Store parameters even if optimization didn't fully converge\n",
    "            # Clip final parameters just to be safe (should be within bounds anyway)\n",
    "            self.parameters = np.clip(opt_result.x, self._lb, self._ub)\n",
    "\n",
    "            if not opt_result.success:\n",
    "                warnings.warn(f\"Optimization did not converge: {opt_result.message}. Parameters may not be optimal.\", RuntimeWarning)\n",
    "\n",
    "        except Exception as e:\n",
    "             warnings.warn(f\"Fatal error during optimization: {e}. Model fitting failed.\", RuntimeWarning)\n",
    "             # Set attributes to indicate failure if optimization crashes\n",
    "             self.parameters = None # Indicate failure\n",
    "             self.std_errors = None\n",
    "             self.log_likelihood = -np.inf\n",
    "             self.results = {\"optim_convergence\": False, \"optim_message\": f\"Optimization Error: {e}\"}\n",
    "             return # Exit fit method if optimization crashes\n",
    "\n",
    "        # --- Post-Optimization Calculations (even if not converged) ---\n",
    "        # Recalculate likelihood details with the final parameters found\n",
    "        final_likelihood_details = self._calculate_likelihood(self.parameters, return_details=True)\n",
    "\n",
    "        # Store positive log-likelihood\n",
    "        self.log_likelihood = -final_likelihood_details[\"LL\"]\n",
    "\n",
    "        # --- Calculate Standard Errors ---\n",
    "        # Calculate SEs based on the parameters found (may be NaN if fit failed badly)\n",
    "        self.std_errors = self._calculate_std_errors(self.parameters)\n",
    "\n",
    "        # --- Calculate Smoothed Probabilities ---\n",
    "        # Need to ensure inputs to smoother are valid\n",
    "        A_final = final_likelihood_details.get(\"A\")\n",
    "        pmat_final = final_likelihood_details.get(\"pmat\")\n",
    "        smoothed_p = None # Initialize\n",
    "        if A_final is None or pmat_final is None or not np.all(np.isfinite(pmat_final)):\n",
    "             warnings.warn(\"Invalid inputs for smoothing after optimization. Smoothed probabilities set to uniform.\", RuntimeWarning)\n",
    "             T_obs = self.ret.shape[0]\n",
    "             smoothed_p = np.ones((T_obs, self.k_states)) / self.k_states\n",
    "        else:\n",
    "             try:\n",
    "                 smoothed_p = self._smooth_probabilities(A_final, pmat_final) # Should be (T, k)\n",
    "             except Exception as smooth_e:\n",
    "                 warnings.warn(f\"Error during smoothing: {smooth_e}. Smoothed probabilities set to uniform.\", RuntimeWarning)\n",
    "                 T_obs = self.ret.shape[0]\n",
    "                 smoothed_p = np.ones((T_obs, self.k_states)) / self.k_states\n",
    "\n",
    "\n",
    "        # --- Prepare results dictionary ---\n",
    "        coef = self.parameters.copy() # Unannualized sigma\n",
    "        se_display = self.std_errors.flatten().copy() if self.std_errors is not None else np.full(len(coef), np.nan) # Flatten SEs\n",
    "\n",
    "        # Handle kbar=1 case for display\n",
    "        if self.kbar == 1:\n",
    "            coef[1] = np.nan\n",
    "            if len(se_display) > 1: se_display[1] = np.nan\n",
    "\n",
    "        # Annualize sigma and its standard error for display coefficients\n",
    "        # SE[f(x)] approx |f'(x)| * SE[x]\n",
    "        # f(sigma_unann) = sigma_unann * sqrt(n_vol) => f' = sqrt(n_vol)\n",
    "        scaling_factor = np.sqrt(self.n_vol)\n",
    "        coef[3] = coef[3] * scaling_factor # Annualize sigma estimate\n",
    "        # Annualize SE only if it's valid\n",
    "        if len(se_display) > 3 and np.isfinite(se_display[3]):\n",
    "             se_display[3] = se_display[3] * scaling_factor\n",
    "        elif len(se_display) > 3:\n",
    "             se_display[3] = np.nan # Keep NaN if original SE was NaN\n",
    "\n",
    "        # Store all results\n",
    "        self.results = {\n",
    "            \"LL\": self.log_likelihood,\n",
    "            \"LLs\": final_likelihood_details.get(\"LLs\"), # Shape (T,)\n",
    "            # Filtered probabilities P(S_t | data_{1:t}) for t=1...T\n",
    "            \"filtered_probabilities\": final_likelihood_details.get(\"pmat\")[1:, :] if final_likelihood_details.get(\"pmat\") is not None else None, # Shape (T, k)\n",
    "            \"smoothed_probabilities\": smoothed_p,  # Shape (T, k_states)\n",
    "            \"transition_matrix\": final_likelihood_details.get(\"A\"),\n",
    "            \"state_vol_multipliers\": final_likelihood_details.get(\"g_m\"),\n",
    "            \"component_matrix\": self._calculate_component_matrix(self.parameters[0]), # Use m0 from estimated params\n",
    "            \"optim_message\": opt_result.message if opt_result else \"Optimization failed before result\",\n",
    "            \"optim_convergence\": opt_result.success if opt_result else False,\n",
    "            \"optim_iter\": opt_result.nit if opt_result and hasattr(opt_result, 'nit') else -1, # L-BFGS-B uses 'nit'\n",
    "            \"parameters\": self.parameters,  # [m0, b, gammak, sigma_unann]\n",
    "            \"std_errors\": self.std_errors,  # SEs for unannualized sigma (n_params x 1)\n",
    "            \"coefficients\": coef,           # Coefs for display (annualized sigma) (n_params,)\n",
    "            \"se_display\": se_display        # SEs for display (annualized sigma) (n_params,)\n",
    "        }\n",
    "        self.coef_names = [\"m0\", \"b\", \"gammak\", \"sigma_annualized\"]\n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Prints a summary table of the estimated model parameters, standard errors,\n",
    "        t-values, and p-values. Uses annualized sigma for display.\n",
    "        \"\"\"\n",
    "        # Check if model has been fitted successfully\n",
    "        if self.parameters is None or not self.results or not self.results.get('optim_convergence', False):\n",
    "            # Added check for convergence status\n",
    "            print(\"Model has not been fitted successfully or did not converge.\")\n",
    "            return None\n",
    "\n",
    "        # Retrieve coefficients and SEs prepared for display\n",
    "        coef = self.results.get(\"coefficients\", np.full(4, np.nan))\n",
    "        se = self.results.get(\"se_display\", np.full(4, np.nan)) # Should be flat\n",
    "        log_likelihood = self.results.get(\"LL\", np.nan)\n",
    "        convergence = self.results.get(\"optim_convergence\", False)\n",
    "        iterations = self.results.get(\"optim_iter\", \"N/A\")\n",
    "        message = self.results.get(\"optim_message\", \"N/A\")\n",
    "\n",
    "        # Calculate t-values and p-values\n",
    "        tval = np.full_like(coef, np.nan)\n",
    "        pval = np.full_like(coef, np.nan)\n",
    "\n",
    "        # Calculate only where SE is valid (not NaN and not near zero)\n",
    "        valid_se_mask = ~np.isnan(se) & (np.abs(se) > 1e-12)\n",
    "        tval[valid_se_mask] = coef[valid_se_mask] / se[valid_se_mask]\n",
    "\n",
    "        # Degrees of freedom: T - number of estimated parameters\n",
    "        # Count non-NaN coefficients as estimated parameters\n",
    "        n_estimated_params = np.sum(~np.isnan(coef))\n",
    "        T = len(self.ret)\n",
    "        df = T - n_estimated_params\n",
    "\n",
    "        # Calculate p-values using t-distribution if df > 0\n",
    "        if df > 0:\n",
    "            # Calculate only where t-value is valid\n",
    "            valid_tval_mask = ~np.isnan(tval)\n",
    "            pval[valid_tval_mask] = 2 * (1 - stats.t.cdf(np.abs(tval[valid_tval_mask]), df=df))\n",
    "        else:\n",
    "            warnings.warn(\"Degrees of freedom <= 0. Cannot calculate p-values.\", RuntimeWarning)\n",
    "\n",
    "        # --- Print Summary ---\n",
    "        print(\"*\" * 76)\n",
    "        print(f\"  Markov Switching Multifractal Model (MSM) - kbar={self.kbar}\")\n",
    "        print(\"*\" * 76)\n",
    "        print(f\"  Log-Likelihood: {log_likelihood:.4f}\")\n",
    "        print(f\"  Observations:   {T}\")\n",
    "        print(f\"  Optimization:   Converged={convergence}, Iterations={iterations}\")\n",
    "        print(f\"  Message:        {message}\")\n",
    "        print(\"-\" * 76)\n",
    "\n",
    "        # Create DataFrame for summary table\n",
    "        summary_data = {\n",
    "            \"Estimate\": coef,\n",
    "            \"Std Error\": se,\n",
    "            \"t-value\": tval,\n",
    "            \"p-value\": pval\n",
    "        }\n",
    "        # Use coef_names stored in self, handle case where it might not exist\n",
    "        coef_names = getattr(self, 'coef_names', [\"m0\", \"b\", \"gammak\", \"sigma_annualized\"])\n",
    "        summary_df = pd.DataFrame(summary_data, index=coef_names)\n",
    "\n",
    "        # Format output nicely\n",
    "        with pd.option_context('display.float_format', '{:,.4f}'.format):\n",
    "            print(summary_df.to_string(na_rep='NaN')) # Use to_string to better handle NaNs\n",
    "        print(\"-\" * 76)\n",
    "\n",
    "        return summary_df\n",
    "\n",
    "    def predict(self, h=None):\n",
    "        \"\"\"\n",
    "        Generates fitted values (h=None) or h-step ahead forecasts.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        h : int, optional\n",
    "            Forecast horizon in periods (matching data frequency).\n",
    "            - If None (default): Returns fitted conditional volatility based on\n",
    "              smoothed probabilities (most likely).\n",
    "            - If h >= 1: Returns h-step ahead volatility forecast based on the\n",
    "              last available filtered probability.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict {'vol': ndarray, 'vol_sq': ndarray}\n",
    "            Annualized conditional volatility and variance (fitted or forecast).\n",
    "            Returns NaNs if model fitting failed or inputs are invalid.\n",
    "        \"\"\"\n",
    "        # Check if model fitting was successful\n",
    "        if self.parameters is None or not self.results or not self.results.get('optim_convergence', False):\n",
    "             # print(\"Model has not been fitted successfully. Cannot predict.\") # Can be noisy\n",
    "             # Return dict with NaNs of expected shape\n",
    "             T = len(self.ret) if hasattr(self, 'ret') and self.ret is not None else 1\n",
    "             shape_out = (1, 1) if h is not None else (T, 1)\n",
    "             return {\n",
    "                 \"vol\": np.full(shape_out, np.nan),\n",
    "                 \"vol_sq\": np.full(shape_out, np.nan)\n",
    "             }\n",
    "\n",
    "        # Retrieve necessary components from results\n",
    "        sigma_unann = self.parameters[3]\n",
    "        A = self.results.get(\"transition_matrix\")\n",
    "        g_m = self.results.get(\"state_vol_multipliers\")\n",
    "        filtered_p = self.results.get(\"filtered_probabilities\") # Shape (T, k)\n",
    "        smoothed_p = self.results.get(\"smoothed_probabilities\") # Shape (T, k)\n",
    "\n",
    "        # --- Input Validation for Prediction ---\n",
    "        if A is None or g_m is None:\n",
    "             warnings.warn(\"Transition matrix or multipliers not available. Cannot predict.\", RuntimeWarning)\n",
    "             T = len(self.ret); shape_out = (1, 1) if h is not None else (T, 1)\n",
    "             return {\"vol\": np.full(shape_out, np.nan), \"vol_sq\": np.full(shape_out, np.nan)}\n",
    "\n",
    "\n",
    "        # Determine which probability matrix to use based on h\n",
    "        if h is None: # Fitted values -> Use smoothed probabilities if available\n",
    "            P_in = smoothed_p\n",
    "            if P_in is None or not np.all(np.isfinite(P_in)):\n",
    "                 warnings.warn(\"Smoothed probabilities invalid/unavailable for fitting. Returning NaNs.\", RuntimeWarning)\n",
    "                 T = len(self.ret)\n",
    "                 return {\"vol\": np.full((T,1), np.nan), \"vol_sq\": np.full((T,1), np.nan)}\n",
    "        else: # Forecasting -> Use last row of filtered probabilities\n",
    "            if not isinstance(h, int) or h < 1:\n",
    "                 raise ValueError(\"Forecast horizon h must be a positive integer.\")\n",
    "            # Check if filtered probabilities are valid\n",
    "            if filtered_p is None or filtered_p.shape[0] == 0 or not np.all(np.isfinite(filtered_p[-1,:])):\n",
    "                 warnings.warn(\"Filtered probabilities invalid/unavailable for forecasting. Returning NaNs.\", RuntimeWarning)\n",
    "                 return {\"vol\": np.array([[np.nan]]), \"vol_sq\": np.array([[np.nan]])}\n",
    "            # Use the last row P(S_T | data_{1:T})\n",
    "            P_in = filtered_p[-1:, :] # Shape (1, k)\n",
    "\n",
    "        # Call the internal prediction function\n",
    "        return self._predict_volatility(P_in, A, g_m, sigma_unann, h=h)\n",
    "\n",
    "\n",
    "    def plot(self, plot_type=\"vol\", use_smoothed=True):\n",
    "        \"\"\"\n",
    "        Plots fitted conditional volatility or variance against a proxy\n",
    "        (absolute or squared returns).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        plot_type : str, 'vol' or 'volsq'\n",
    "            Whether to plot volatility or variance.\n",
    "        use_smoothed : bool\n",
    "            If True, uses smoothed probabilities for fitted values.\n",
    "            If False, uses filtered probabilities.\n",
    "        \"\"\"\n",
    "        # Check model state\n",
    "        if self.parameters is None or not self.results:\n",
    "            print(\"Model has not been fitted or fitting failed. Cannot plot.\")\n",
    "            return\n",
    "        if not self.results.get('optim_convergence', False):\n",
    "             print(\"Model did not converge. Plotting may be based on non-optimal parameters.\")\n",
    "\n",
    "\n",
    "        if plot_type not in [\"vol\", \"volsq\"]:\n",
    "            raise ValueError(\"plot_type must be either 'vol' or 'volsq'\")\n",
    "\n",
    "        # --- Get Fitted Values ---\n",
    "        # Retrieve necessary components\n",
    "        sigma_unann = self.parameters[3]\n",
    "        A = self.results.get(\"transition_matrix\")\n",
    "        g_m = self.results.get(\"state_vol_multipliers\")\n",
    "\n",
    "        # Select probabilities based on use_smoothed flag\n",
    "        if use_smoothed:\n",
    "            P_fit = self.results.get(\"smoothed_probabilities\")\n",
    "            title_suffix = \"(Smoothed)\"\n",
    "            if P_fit is None or not np.all(np.isfinite(P_fit)):\n",
    "                 print(\"Smoothed probabilities not available or invalid. Cannot plot.\")\n",
    "                 return\n",
    "        else:\n",
    "            P_fit = self.results.get(\"filtered_probabilities\")\n",
    "            title_suffix = \"(Filtered)\"\n",
    "            if P_fit is None or not np.all(np.isfinite(P_fit)):\n",
    "                 print(\"Filtered probabilities not available or invalid. Cannot plot.\")\n",
    "                 return\n",
    "\n",
    "        # Calculate fitted values using the internal method (h=None)\n",
    "        pred = self._predict_volatility(P_fit, A, g_m, sigma_unann, h=None)\n",
    "\n",
    "        # Check if prediction was successful\n",
    "        if pred is None or 'vol' not in pred or np.all(np.isnan(pred['vol'])):\n",
    "             print(\"Fitted volatility could not be calculated. Cannot plot.\")\n",
    "             return\n",
    "\n",
    "        # Ensure returns have the same length as fitted vol\n",
    "        T_fit = pred['vol'].shape[0]\n",
    "        # Use self.ret which should be (T, 1)\n",
    "        returns_plot = self.ret[:T_fit, 0] # Match length and get 1D array\n",
    "\n",
    "        # --- Create Plot ---\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        if plot_type == \"vol\":\n",
    "            fitted_values = pred[\"vol\"][:, 0] # Get 1D array\n",
    "            # Calculate annualized absolute returns as proxy\n",
    "            data_proxy = np.abs(returns_plot * np.sqrt(self.n_vol))\n",
    "            proxy_label = \"Annualized Abs Returns\"\n",
    "            plot_title = f\"Fitted Conditional Volatility vs Absolute Returns {title_suffix}\"\n",
    "            ylabel = \"Annualized Volatility\"\n",
    "        else: # volsq\n",
    "            fitted_values = pred[\"vol_sq\"][:, 0] # Get 1D array\n",
    "            # Calculate annualized squared returns as proxy\n",
    "            data_proxy = np.square(returns_plot * np.sqrt(self.n_vol))\n",
    "            proxy_label = \"Annualized Squared Returns\"\n",
    "            plot_title = f\"Fitted Conditional Variance vs Squared Returns {title_suffix}\"\n",
    "            ylabel = \"Annualized Variance\"\n",
    "\n",
    "        # Plot fitted values and proxy\n",
    "        plt.plot(fitted_values, label=f\"Fitted Annualized {plot_type.capitalize()} {title_suffix}\", color='blue', linewidth=1.5)\n",
    "        plt.plot(data_proxy, label=proxy_label, alpha=0.5, color='orange', linestyle='-', linewidth=0.8)\n",
    "        plt.title(plot_title)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(\"Time Period\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def decompose(self, use_smoothed=True):\n",
    "        \"\"\"\n",
    "        Decomposes volatility into contributions from each frequency component M_k.\n",
    "        Calculates E[M_k | data] = m0*P(M_k=m0|data) + (2-m0)*P(M_k=2-m0|data).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        use_smoothed : bool, optional (default True)\n",
    "            Use smoothed or filtered probabilities.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray (T x kbar) or None if failed.\n",
    "        \"\"\"\n",
    "        # Check model state\n",
    "        if self.parameters is None or not self.results:\n",
    "            print(\"Model has not been fitted or fitting failed.\")\n",
    "            return None\n",
    "        # Allow decomposition even if not converged, but warn\n",
    "        if not self.results.get('optim_convergence', False):\n",
    "             print(\"Warning: Model did not converge. Decomposition based on non-optimal parameters.\")\n",
    "\n",
    "\n",
    "        m0 = self.parameters[0]\n",
    "        Mmat = self.results.get(\"component_matrix\") # k_states x kbar\n",
    "        if Mmat is None:\n",
    "             print(\"Component matrix not available. Cannot decompose.\")\n",
    "             return None\n",
    "\n",
    "        # Select probabilities\n",
    "        if use_smoothed:\n",
    "            P = self.results.get(\"smoothed_probabilities\") # T x k_states\n",
    "            prob_type = \"Smoothed\"\n",
    "            if P is None or not np.all(np.isfinite(P)):\n",
    "                 print(\"Smoothed probabilities invalid/unavailable. Cannot decompose.\")\n",
    "                 return None\n",
    "        else:\n",
    "            P = self.results.get(\"filtered_probabilities\") # T x k_states\n",
    "            prob_type = \"Filtered\"\n",
    "            if P is None or not np.all(np.isfinite(P)):\n",
    "                 print(\"Filtered probabilities invalid/unavailable. Cannot decompose.\")\n",
    "                 return None\n",
    "\n",
    "        # Calculate marginal probabilities P(M_k = m0 | data) -> shape (T, kbar)\n",
    "        try:\n",
    "             p_m0_marginals = self._calculate_marginal_probabilities(P, m0, Mmat)\n",
    "        except Exception as e:\n",
    "             print(f\"Error calculating marginal probabilities: {e}\")\n",
    "             return None\n",
    "\n",
    "        # Calculate E[M_k | data] = m0 * P(M_k=m0) + m1 * (1 - P(M_k=m0))\n",
    "        m1 = 2.0 - m0\n",
    "        expected_M_k = m0 * p_m0_marginals + m1 * (1.0 - p_m0_marginals)\n",
    "\n",
    "        # Ensure result is finite\n",
    "        if not np.all(np.isfinite(expected_M_k)):\n",
    "             warnings.warn(\"Non-finite values in expected components. Replacing with average.\", RuntimeWarning)\n",
    "             avg_val = m0 * 0.5 + m1 * 0.5 # Default expectation if probs are 0.5\n",
    "             expected_M_k = np.nan_to_num(expected_M_k, nan=avg_val, posinf=m0, neginf=m1)\n",
    "\n",
    "        return expected_M_k  # Shape (T, kbar)\n",
    "\n",
    "    def plot_components(self, use_smoothed=True):\n",
    "        \"\"\"\n",
    "        Plots the decomposed volatility components E[M_k | data] over time.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        use_smoothed : bool, optional (default True)\n",
    "            Passed to the decompose method.\n",
    "        \"\"\"\n",
    "        if self.parameters is None or not self.results:\n",
    "            print(\"Model has not been fitted or fitting failed.\")\n",
    "            return\n",
    "\n",
    "        # Get decomposed components\n",
    "        expected_M_k = self.decompose(use_smoothed=use_smoothed)\n",
    "        if expected_M_k is None:\n",
    "            print(\"Decomposition failed. Cannot plot components.\")\n",
    "            return\n",
    "\n",
    "        prob_type = \"Smoothed\" if use_smoothed else \"Filtered\"\n",
    "        T, num_components = expected_M_k.shape\n",
    "\n",
    "        # Adjust plotting layout based on number of components\n",
    "        if num_components == 0:\n",
    "             print(\"No components to plot (kbar=0?).\")\n",
    "             return\n",
    "        elif num_components == 1:\n",
    "             fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "             axes = [ax] # Make it iterable\n",
    "        else:\n",
    "             # Create subplots, sharing the x-axis\n",
    "             fig, axes = plt.subplots(num_components, 1, figsize=(12, 2.5 * num_components), sharex=True)\n",
    "\n",
    "        fig.suptitle(f\"Expected Volatility Components E[M_k | Data] ({prob_type})\", fontsize=14)\n",
    "\n",
    "        # Get m0 and m1 for plotting horizontal lines\n",
    "        m0 = self.parameters[0]\n",
    "        m1 = 2.0 - m0\n",
    "\n",
    "        # Plot each component's expectation\n",
    "        for k in range(num_components):\n",
    "            ax = axes[k]\n",
    "            # Plot E[M_{k+1} | Data]\n",
    "            ax.plot(expected_M_k[:, k], label=f\"E[M$_{k + 1}$ | Data]\", linewidth=1.5)\n",
    "            # Add horizontal lines for m0 and m1\n",
    "            ax.axhline(m0, color='r', linestyle='--', alpha=0.7, label=f'$m_0 \\\\approx {m0:.3f}$')\n",
    "            ax.axhline(m1, color='g', linestyle='--', alpha=0.7, label=f'$m_1 \\\\approx {m1:.3f}$')\n",
    "\n",
    "            # Set titles and labels\n",
    "            ax.set_title(f\"Component M$_{k + 1}$\")\n",
    "            ax.set_ylabel(\"$E[M_k]$\")\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            ax.legend(loc='best')\n",
    "            # Set y-limits for better visualization\n",
    "            ax.set_ylim(min(m0, m1) - 0.1, max(m0, m1) + 0.1)\n",
    "\n",
    "        # Set x-label only on the bottom-most plot\n",
    "        axes[-1].set_xlabel(\"Time Period\")\n",
    "\n",
    "        # Adjust layout to prevent title overlap\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # [left, bottom, right, top]\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Evaluation Pipeline Script\n",
    "# =============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your 5-minute data CSV\n",
    "# >>>>>>>> IMPORTANT: Update this path <<<<<<<<<<<\n",
    "root = Path().resolve().parent # Assuming script is one level down from project root\n",
    "data_path = root / 'data' / 'csi2007.csv' # Path from user's code\n",
    "data_file = data_path\n",
    "\n",
    "# MSM Model Parameters\n",
    "# --- FIX 1: Reduce KBAR significantly ---\n",
    "KBAR = 3 # Start with 3 components (8 states), try 2 if this still fails\n",
    "# --- FIX 2: Use standard annualization factor ---\n",
    "ANNUALIZATION_FACTOR = 252 # Trading days in a year\n",
    "\n",
    "# Rolling Window Parameters\n",
    "INITIAL_ESTIMATION_YEARS = 3 # Years of data for the first fit\n",
    "EVALUATION_START_YEAR = 2008 # Start forecasting from this year\n",
    "MIN_ROLLING_WINDOW_DAYS = 200 # Minimum number of days required in the rolling window\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def calculate_realized_volatility(df_5min_returns, annualization_factor=252):\n",
    "    \"\"\"Calculates daily realized variance and volatility from 5-min returns.\"\"\"\n",
    "    if not isinstance(df_5min_returns.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Input DataFrame must have a DatetimeIndex.\")\n",
    "    df_copy = df_5min_returns.copy()\n",
    "    # Ensure log returns are numeric and handle potential infinities\n",
    "    df_copy['log_ret_5min'] = pd.to_numeric(df_copy['log_ret_5min'], errors='coerce')\n",
    "    df_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_copy.dropna(subset=['log_ret_5min'], inplace=True) # Drop NaNs again after coercion/inf handling\n",
    "\n",
    "    df_copy['log_ret_5min_sq'] = df_copy['log_ret_5min']**2\n",
    "    # Ensure sum only happens if there are valid squared returns for the day\n",
    "    realized_variance_daily = df_copy['log_ret_5min_sq'].resample('D').sum(min_count=1) # Require at least 1 valid sq return\n",
    "    realized_volatility_daily = np.sqrt(realized_variance_daily * annualization_factor)\n",
    "    realized_volatility_daily.name = 'realized_volatility'\n",
    "    return realized_volatility_daily\n",
    "\n",
    "def calculate_daily_returns(df_5min, price_col='close'):\n",
    "    \"\"\"Calculates daily log returns from 5-min close prices.\"\"\"\n",
    "    if not isinstance(df_5min.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Input DataFrame must have a DatetimeIndex.\")\n",
    "    # Ensure price column is numeric\n",
    "    df_5min[price_col] = pd.to_numeric(df_5min[price_col], errors='coerce')\n",
    "    # Use 'B' frequency to only get business day closes\n",
    "    daily_close = df_5min[price_col].resample('B').last()\n",
    "    # Forward fill missing daily closes (e.g., holidays) before calculating returns\n",
    "    daily_close = daily_close.ffill()\n",
    "    daily_log_returns = np.log(daily_close).diff()\n",
    "    daily_log_returns.name = 'log_ret_daily'\n",
    "    return daily_log_returns\n",
    "\n",
    "def evaluate_forecasts(comparison_df):\n",
    "    \"\"\"Calculates and prints evaluation metrics.\"\"\"\n",
    "    required_cols = ['realized_volatility', 'msm_forecast']\n",
    "    if not all(col in comparison_df.columns for col in required_cols):\n",
    "        print(\"Error: Missing required columns for evaluation.\")\n",
    "        return None\n",
    "    eval_df = comparison_df[required_cols].dropna()\n",
    "    if eval_df.empty or len(eval_df) < 2: # Need at least 2 points for regression\n",
    "        print(\"No overlapping data or insufficient data available for evaluation after dropping NaNs.\")\n",
    "        return None\n",
    "\n",
    "    realized_vol = eval_df['realized_volatility']\n",
    "    forecast_vol = eval_df['msm_forecast']\n",
    "\n",
    "    # Add check for non-finite values before metrics\n",
    "    finite_mask = np.isfinite(realized_vol) & np.isfinite(forecast_vol)\n",
    "    if not finite_mask.all():\n",
    "        print(\"Warning: Non-finite values found in realized or forecast volatility. Filtering before evaluation.\")\n",
    "        realized_vol = realized_vol[finite_mask]\n",
    "        forecast_vol = forecast_vol[finite_mask]\n",
    "        if len(realized_vol) < 2:\n",
    "             print(\"Insufficient finite data points remaining for evaluation.\")\n",
    "             return None\n",
    "\n",
    "\n",
    "    # --- Metrics ---\n",
    "    if len(realized_vol) == 0 or len(forecast_vol) == 0:\n",
    "        print(\"Cannot calculate metrics: No valid data points.\")\n",
    "        return eval_df\n",
    "\n",
    "    try:\n",
    "        mse = mean_squared_error(realized_vol, forecast_vol)\n",
    "        mae = np.mean(np.abs(realized_vol - forecast_vol))\n",
    "        rmse = np.sqrt(mse)\n",
    "        epsilon = 1e-12\n",
    "        realized_var = np.maximum(realized_vol, 0)**2 + epsilon\n",
    "        forecast_var = np.maximum(forecast_vol, 0)**2 + epsilon\n",
    "        forecast_var = np.maximum(forecast_var, epsilon)\n",
    "        valid_qlike_mask = (forecast_var > 0) & (realized_var >= 0)\n",
    "        if np.sum(valid_qlike_mask) > 0:\n",
    "            qlike = np.mean(np.log(forecast_var[valid_qlike_mask]) + realized_var[valid_qlike_mask] / forecast_var[valid_qlike_mask])\n",
    "        else:\n",
    "            qlike = np.nan\n",
    "\n",
    "        print(\"\\n--- Forecast Evaluation Metrics ---\")\n",
    "        print(f\"  RMSE: {rmse:.6f}\")\n",
    "        print(f\"  MAE:  {mae:.6f}\")\n",
    "        print(f\"  MSE:  {mse:.6f}\")\n",
    "        print(f\"  QLIKE:{qlike:.4f} (Lower is better)\")\n",
    "\n",
    "    except Exception as metric_e:\n",
    "        print(f\"Error calculating basic metrics: {metric_e}\")\n",
    "        return eval_df\n",
    "\n",
    "\n",
    "    # --- Mincer-Zarnowitz Regression ---\n",
    "    if np.std(forecast_vol) > 1e-9 and len(forecast_vol) > 1:\n",
    "        X = sm.add_constant(forecast_vol)\n",
    "        y = realized_vol\n",
    "        try:\n",
    "            mz_model = sm.OLS(y, X).fit()\n",
    "            print(\"\\n--- Mincer-Zarnowitz Regression (Realized ~ const + Forecast) ---\")\n",
    "            print(mz_model.summary())\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCould not perform Mincer-Zarnowitz regression: {e}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping Mincer-Zarnowitz regression: Insufficient variation in forecasts or too few data points.\")\n",
    "\n",
    "\n",
    "    # --- Plotting ---\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        # Use original eval_df index but filtered data for plotting\n",
    "        plot_index = eval_df.index[finite_mask] if 'finite_mask' in locals() and finite_mask.any() else eval_df.index\n",
    "        plt.plot(plot_index, realized_vol, label='Realized Volatility (Ground Truth)', alpha=0.7, linewidth=1.5, color='black')\n",
    "        plt.plot(plot_index, forecast_vol, label=f'MSM Forecast (k={KBAR})', alpha=0.8, linewidth=1.2, color='red')\n",
    "        plt.title('MSM Daily Volatility Forecast vs. Realized Volatility')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Annualized Volatility')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as plot_e:\n",
    "        print(f\"Error during plotting: {plot_e}\")\n",
    "\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "\n",
    "# 1. Load and Prepare 5-minute Data\n",
    "print(f\"Loading 5-minute data from: {data_file}...\")\n",
    "try:\n",
    "    df_5min = pd.read_csv(\n",
    "        data_file,\n",
    "        index_col='datetime',\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    if df_5min.empty: print(f\"Error: DataFrame is empty after loading data from {data_file}.\"); sys.exit(1)\n",
    "    df_5min.sort_index(inplace=True)\n",
    "    print(f\"Loaded {len(df_5min)} records.\")\n",
    "    print(f\"Data range: {df_5min.index.min()} to {df_5min.index.max()}\")\n",
    "    required_cols_5min = ['close']\n",
    "    if not all(col in df_5min.columns for col in required_cols_5min):\n",
    "         missing_cols = [col for col in required_cols_5min if col not in df_5min.columns]\n",
    "         print(f\"Error: Missing required columns in loaded 5-min data: {missing_cols}\"); sys.exit(1)\n",
    "except FileNotFoundError: print(f\"Error: Data file not found at {data_file}\"); raise\n",
    "except Exception as e: print(f\"Error loading or performing initial checks on data: {e}\"); raise\n",
    "\n",
    "# 2. Calculate 5-min Returns\n",
    "print(\"Calculating 5-minute log returns...\")\n",
    "if 'close' not in df_5min.columns: print(\"Error: 'close' column not found.\"); sys.exit(1)\n",
    "df_5min['close'] = pd.to_numeric(df_5min['close'], errors='coerce')\n",
    "df_5min.dropna(subset=['close'], inplace=True)\n",
    "df_5min['log_ret_5min'] = np.log(df_5min['close']).diff()\n",
    "df_5min = df_5min.dropna(subset=['log_ret_5min'])\n",
    "\n",
    "# 3. Calculate Daily Realized Volatility (Ground Truth)\n",
    "print(\"Calculating daily realized volatility...\")\n",
    "daily_rv = calculate_realized_volatility(df_5min, ANNUALIZATION_FACTOR)\n",
    "\n",
    "# 4. Calculate Daily Returns (for MSM fitting)\n",
    "print(\"Calculating daily log returns...\")\n",
    "daily_returns = calculate_daily_returns(df_5min)\n",
    "\n",
    "# 5. Align Data into a Daily DataFrame\n",
    "print(\"Aligning daily data...\")\n",
    "df_daily = pd.DataFrame(index=daily_returns.index)\n",
    "df_daily = df_daily.join(daily_returns).join(daily_rv)\n",
    "df_daily = df_daily.dropna(subset=['log_ret_daily', 'realized_volatility'])\n",
    "df_daily = df_daily[np.isfinite(df_daily['log_ret_daily'])]\n",
    "print(f\"Created daily DataFrame with {len(df_daily)} trading days.\")\n",
    "print(f\"Daily data range: {df_daily.index.min().date()} to {df_daily.index.max().date()}\")\n",
    "\n",
    "# 6. Rolling Window Forecasting Setup\n",
    "print(\"\\nSetting up rolling window forecasting...\")\n",
    "try:\n",
    "    initial_estimation_start_date = df_daily.index.min()\n",
    "    initial_estimation_end_date_target = initial_estimation_start_date + pd.DateOffset(years=INITIAL_ESTIMATION_YEARS) - pd.Timedelta(days=1)\n",
    "    valid_end_dates = df_daily.index[df_daily.index <= initial_estimation_end_date_target]\n",
    "    if valid_end_dates.empty: raise ValueError(\"Cannot determine initial estimation end date.\")\n",
    "    initial_estimation_end_date = valid_end_dates[-1]\n",
    "\n",
    "    initial_window_data = df_daily.loc[initial_estimation_start_date:initial_estimation_end_date]\n",
    "    initial_window_returns = initial_window_data['log_ret_daily'].dropna().values\n",
    "    initial_window_size_days = len(initial_window_returns) # Use actual returns count\n",
    "    print(f\"Initial estimation window size: {initial_window_size_days} days\")\n",
    "\n",
    "    eval_start_lookup = df_daily.index[df_daily.index >= f\"{EVALUATION_START_YEAR}-01-01\"]\n",
    "    if eval_start_lookup.empty: raise ValueError(f\"No data found starting from {EVALUATION_START_YEAR}\")\n",
    "    evaluation_start_date = eval_start_lookup[0]\n",
    "    evaluation_dates = df_daily.loc[evaluation_start_date:].index\n",
    "    print(f\"Initial estimation period: {initial_estimation_start_date.date()} to {initial_estimation_end_date.date()}\")\n",
    "    print(f\"Evaluation period: {evaluation_start_date.date()} to {df_daily.index.max().date()} ({len(evaluation_dates)} days)\")\n",
    "\n",
    "    if initial_window_size_days < MIN_ROLLING_WINDOW_DAYS:\n",
    "        print(f\"Warning: Initial estimation window ({initial_window_size_days} days) is smaller than MIN_ROLLING_WINDOW_DAYS ({MIN_ROLLING_WINDOW_DAYS}).\")\n",
    "\n",
    "except (IndexError, ValueError) as e: print(f\"Error setting up estimation/evaluation periods: {e}. Check data range and EVALUATION_START_YEAR.\"); raise\n",
    "except Exception as e: print(f\"Unexpected error setting up estimation/evaluation periods: {e}\"); raise\n",
    "\n",
    "# --- DEBUG: Test Fit on Initial Window ---\n",
    "print(\"\\n--- Attempting fit on INITIAL estimation window ---\")\n",
    "if len(initial_window_returns) >= MIN_ROLLING_WINDOW_DAYS and np.all(np.isfinite(initial_window_returns)):\n",
    "    try:\n",
    "        print(f\"Fitting MSM(k={KBAR}) on initial window ({len(initial_window_returns)} days)...\")\n",
    "        # Pass necessary args, removed verbose\n",
    "        initial_model = MSM(ret=initial_window_returns, kbar=KBAR, n_vol=ANNUALIZATION_FACTOR)\n",
    "        if initial_model.parameters is None or not initial_model.results.get('optim_convergence', False):\n",
    "            print(\"--- Initial Fit FAILED or did not converge ---\")\n",
    "            if hasattr(initial_model, 'results') and initial_model.results:\n",
    "                 print(f\"   Convergence: {initial_model.results.get('optim_convergence', 'N/A')}\")\n",
    "                 print(f\"   Message: {initial_model.results.get('optim_message', 'N/A')}\")\n",
    "            else:\n",
    "                 print(\"   MSM results object not available.\")\n",
    "        else:\n",
    "            print(\"--- Initial Fit SUCCESSFUL ---\")\n",
    "            print(f\"   LogLik: {initial_model.log_likelihood:.4f}\")\n",
    "            print(f\"   Params (unannualized sigma): {initial_model.parameters}\")\n",
    "    except Exception as initial_e:\n",
    "        print(f\"--- ERROR during initial fit attempt: {type(initial_e).__name__} - {initial_e} ---\")\n",
    "        print(traceback.format_exc())\n",
    "else:\n",
    "    print(f\"--- Skipping initial fit test: Window size {len(initial_window_returns)} < {MIN_ROLLING_WINDOW_DAYS} or non-finite data ---\")\n",
    "# --- End DEBUG Block ---\n",
    "\n",
    "\n",
    "print(\"\\nStarting rolling window forecasting loop...\")\n",
    "msm_forecasts = []\n",
    "forecast_dates = []\n",
    "failed_fits = 0 # Counter for failed MSM fits\n",
    "start_time = time.time()\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "for i in tqdm(range(len(evaluation_dates)), desc=\"Rolling Forecast\"):\n",
    "    forecast_for_date = evaluation_dates[i]\n",
    "    forecast_value = np.nan # Default forecast value if anything fails\n",
    "\n",
    "    try:\n",
    "        # --- Window Selection ---\n",
    "        window_end_idx_loc = df_daily.index.get_loc(forecast_for_date) - 1\n",
    "        if window_end_idx_loc < 0: continue\n",
    "\n",
    "        window_start_idx_loc = max(0, window_end_idx_loc - initial_window_size_days + 1)\n",
    "        current_window_data = df_daily.iloc[window_start_idx_loc : window_end_idx_loc + 1]\n",
    "        current_returns = current_window_data['log_ret_daily'].dropna().values\n",
    "\n",
    "        # --- Check Window Size ---\n",
    "        if len(current_returns) < MIN_ROLLING_WINDOW_DAYS:\n",
    "            msm_forecasts.append(np.nan)\n",
    "            forecast_dates.append(forecast_for_date)\n",
    "            continue\n",
    "\n",
    "        # --- Attempt MSM Fit ---\n",
    "        rolling_model = None\n",
    "        if not np.all(np.isfinite(current_returns)):\n",
    "             failed_fits += 1\n",
    "             forecast_value = np.nan\n",
    "        else:\n",
    "            # --- Fit Model (removed verbose) ---\n",
    "            rolling_model = MSM(ret=current_returns, kbar=KBAR, n_vol=ANNUALIZATION_FACTOR)\n",
    "\n",
    "            # --- Check Fit Success ---\n",
    "            if rolling_model.parameters is None or not rolling_model.results.get('optim_convergence', False):\n",
    "                failed_fits += 1\n",
    "                forecast_value = np.nan\n",
    "            else:\n",
    "                # --- Attempt Prediction ---\n",
    "                prediction = rolling_model.predict(h=1)\n",
    "                if prediction and 'vol' in prediction and not np.isnan(prediction['vol'][0, 0]):\n",
    "                    forecast_value = prediction['vol'][0, 0]\n",
    "                else:\n",
    "                    forecast_value = np.nan # Ensure NaN if predict fails\n",
    "\n",
    "    except Exception as e:\n",
    "        # --- Log the specific error for this iteration ---\n",
    "        if failed_fits < 10: # Only print first few errors\n",
    "             print(f\"\\nERROR during loop for {forecast_for_date.date()}: {type(e).__name__} - {e}\")\n",
    "        elif failed_fits == 10:\n",
    "             print(\"\\n...(suppressing further error messages from loop)...\")\n",
    "        failed_fits += 1\n",
    "        forecast_value = np.nan\n",
    "\n",
    "    # --- Store Results for this date ---\n",
    "    msm_forecasts.append(forecast_value)\n",
    "    forecast_dates.append(forecast_for_date)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nRolling forecast loop finished in {end_time - start_time:.2f} seconds.\")\n",
    "if failed_fits > 0:\n",
    "    print(f\"Warning: The MSM model fit failed, did not converge, or encountered an error for {failed_fits} out of {len(evaluation_dates)} windows.\")\n",
    "\n",
    "# 7. Combine Forecasts and Evaluate\n",
    "print(\"Combining forecasts with daily data...\")\n",
    "if not forecast_dates: print(\"Error: No forecast dates were generated.\"); sys.exit(1)\n",
    "forecast_series = pd.Series(msm_forecasts, index=pd.Index(forecast_dates, name='datetime'), name='msm_forecast')\n",
    "if forecast_series.index.has_duplicates:\n",
    "    print(\"Warning: Duplicate forecast dates found. Keeping last.\"); forecast_series = forecast_series[~forecast_series.index.duplicated(keep='last')]\n",
    "df_daily_eval = pd.merge(df_daily, forecast_series, left_index=True, right_index=True, how='left')\n",
    "df_eval_period = df_daily_eval.loc[evaluation_start_date:].copy()\n",
    "valid_forecast_count = df_eval_period['msm_forecast'].count()\n",
    "print(f\"Generated {valid_forecast_count} valid forecasts out of {len(df_eval_period)} evaluation days.\")\n",
    "\n",
    "if valid_forecast_count > 0:\n",
    "    evaluation_results_df = evaluate_forecasts(df_eval_period)\n",
    "    if evaluation_results_df is not None: print(\"\\n--- Sample of Forecasts vs. Realized Volatility ---\\n\", evaluation_results_df.head())\n",
    "    else: print(\"\\nEvaluation could not be completed.\")\n",
    "else: print(\"\\nNo valid forecasts were generated, skipping evaluation.\")\n",
    "\n",
    "print(\"\\nPipeline finished.\")"
   ],
   "id": "341ba9f56827d129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd09a54a5ddac75",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
